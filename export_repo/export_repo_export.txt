<codebase_context>

<dirtree: export_repo>
|-- configs (1039 lines)
|   |-- autocrop.json (10)
|   |-- autocropper.json (12)
|   |-- bulk_dl.json (14)
|   |-- cosm-c360-data.json (12)
|   |-- cosm-c360-tools.json (12)
|   |-- export_repo.json (13)
|   |-- ezmd.json (11)
|   |-- ezprompt.json (11)
|   |-- h5merge-mini.json (12)
|   |-- h5merge.json (12)
|   |-- h5pull.json (11)
|   |-- hFormer0-serve.json (10)
|   |-- ibrida.json (13)
|   |-- ibridaDB_v0r1_export.json (13)
|   |-- ibridaDB_v0r1_ingest.json (10)
|   |-- ibridaDB_v0rX.json (12)
|   |-- ibrida_analysis.json (13)
|   |-- ibrida_autocrop.json (13)
|   |-- ladybird_data.json (13)
|   |-- metaformer.json (11)
|   |-- metaformer1.json (12)
|   |-- metaformer2.json (11)
|   |-- model-explorer.json (13)
|   |-- nextjs.json (8)
|   |-- polliFormer-COPAP.json (12)
|   |-- polliFormer-Dyn.json (18)
|   |-- polliFormer-DynSlim.json (13)
|   |-- polliFormer-J25_0e0.json (13)
|   |-- polliFormer-aug.json (11)
|   |-- polliFormer-autobatch.json (13)
|   |-- polliFormer-autoresume.json (15)
|   |-- polliFormer-autoscale.json (20)
|   |-- polliFormer-blade-angio-0.json (14)
|   |-- polliFormer-buildData.json (12)
|   |-- polliFormer-classification0.json (21)
|   |-- polliFormer-classification1.json (21)
|   |-- polliFormer-codeOnly.json (11)
|   |-- polliFormer-configData.json (12)
|   |-- polliFormer-configModel.json (14)
|   |-- polliFormer-configModelMini.json (13)
|   |-- polliFormer-data.json (12)
|   |-- polliFormer-debugCOPAP.json (13)
|   |-- polliFormer-doxDyn.json (16)
|   |-- polliFormer-gradnorm.json (25)
|   |-- polliFormer-gradnorm2.json (27)
|   |-- polliFormer-gradnormSlim.json (16)
|   |-- polliFormer-gradnormSlim2.json (14)
|   |-- polliFormer-hierarchyXL.json (17)
|   |-- polliFormer-logging.json (14)
|   |-- polliFormer-loggingSlim.json (14)
|   |-- polliFormer-loss.json (26)
|   |-- polliFormer-loss2.json (21)
|   |-- polliFormer-loss3.json (18)
|   |-- polliFormer-mFormerV1.json (18)
|   |-- polliFormer-meta.json (18)
|   |-- polliFormer-metrics.json (14)
|   |-- polliFormer-models-codeOnly.json (11)
|   |-- polliFormer-models.json (15)
|   |-- polliFormer-modelsDyn.json (16)
|   |-- polliFormer-modelsPruned-codeOnly.json (11)
|   |-- polliFormer-modelsPrunedInv-codeOnly.json (11)
|   |-- polliFormer-modelsSlim.json (18)
|   |-- polliFormer-modelsSlim2.json (16)
|   |-- polliFormer-paramFilters.json (16)
|   |-- polliFormer-serve.json (10)
|   |-- polliFormer-tests.json (12)
|   |-- polliFormer-train.json (16)
|   |-- polliFormer-utils.json (11)
|   |-- polliFormer.json (13)
|   |-- polliOS-codeOnly.json (13)
|   |-- polliOS.json (13)
|   |-- sam2.json (12)
|   |-- sam2_demo.json (15)
|   \-- wandb_sweep.json (12)
|-- export_repo_to_txt.py (573)
|-- new_export_repo_prompts.xml (750)
\-- readme.md (184)
</dirtree: export_repo>

<file: new_export_repo_prompts.xml>
<codebase>
<export_repo_to_txt.py>
import platform
import os
import json
import sys
import nbformat
from nbconvert import MarkdownExporter
from nbconvert.preprocessors import ClearOutputPreprocessor

class PathConverter:
    @staticmethod
    def to_system_path(path):
        """Convert path to the current system's format."""
        if platform.system() == "Windows":
            # Convert forward slashes to backslashes and handle drive letter
            if path.startswith("/"):
                # Remove leading slash and convert to Windows path
                path = path.lstrip("/")
                if ":" not in path:  # If no drive letter, assume C:
                    path = "C:\\" + path
            return path.replace("/", "\\")
        else:
            # Convert backslashes to forward slashes for Unix-like systems
            return path.replace("\\", "/")

    @staticmethod
    def normalize_config_paths(config):
        """Normalize all paths in config to system-specific format."""
        if 'repo_root' in config:
            base_path = get_base_path()
            if platform.system() == "Windows":
                # Replace Unix-style base paths with Windows GitHub path
                for unix_path in ["/home/caleb/repo", "/home/caleb/Documents/GitHub/", "/Users/caleb/Documents/GitHub"]:
                    if config['repo_root'].startswith(unix_path):
                        relative_path = config['repo_root'][len(unix_path):].lstrip("/")
                        config['repo_root'] = os.path.join(base_path, relative_path)
                        break
            
            config['repo_root'] = PathConverter.to_system_path(config['repo_root'])
        
        # Convert paths in lists
        for key in ['dirs_to_traverse', 'subdirs_to_exclude', 'files_to_exclude']:
            if key in config and isinstance(config[key], list):
                config[key] = [PathConverter.to_system_path(p) for p in config[key]]
        
        return config

class RepoExporter:
    def __init__(self, config):
        # Convert all paths in config to system-specific format
        config = PathConverter.normalize_config_paths(config)
        self.repo_root = config['repo_root']
        self.export_name = config['export_name']
        self.delimiter = config['delimiter']
        self.dirs_to_traverse = config['dirs_to_traverse']
        self.include_top_level_files = config['include_top_level_files']
        self.included_extensions = config['included_extensions']
        self.subdirs_to_exclude = config.get('subdirs_to_exclude', [])
        self.files_to_exclude = config.get('files_to_exclude', [])
        self.depth = config.get('depth', -1)  # Default to -1 for full traversal
        self.dump_config = config.get('dump_config', False)
        self.exhaustive_dir_tree = config.get('exhaustive_dir_tree', False)
        self.blacklisted_dirs = ['__pycache__']  # Blacklist of subdirs to always omit
        self.blacklisted_dirs.extend(['.git', '.venv', '.vscode'])  # Add common hidden dirs
        self.blacklisted_files = ['uv.lock', 'LICENSE']  # Blacklist of files to always omit
        self.files_to_include = config.get('files_to_include', [])  # Additional files to include explicitly
        self.output_file = self.get_output_file_path()
        self.files_to_exclude.append(os.path.basename(self.output_file))  # Add output file to exclude list
        self.always_exclude_patterns = config.get('always_exclude_patterns', ['export.txt'])
        self.exported_files_count = {}
        self.dirs_for_tree = config.get('dirs_for_tree', [])
        self.total_lines = 0
        self.line_counts_by_file = {}  # Store line counts by relative file path
        self.line_counts_by_dir = {}   # Store aggregated line counts by directory

    def convert_ipynb_to_md(self, notebook_content):
        notebook = nbformat.reads(notebook_content, as_version=4)
        
        # Clear outputs
        clear_output = ClearOutputPreprocessor()
        clear_output.preprocess(notebook, {})
        
        markdown_exporter = MarkdownExporter()
        markdown_content, _ = markdown_exporter.from_notebook_node(notebook)
        return markdown_content
    
    def get_output_file_path(self):
        if os.path.isabs(self.export_name):
            return PathConverter.to_system_path(self.export_name)
        else:
            return PathConverter.to_system_path(os.path.join(self.repo_root, self.export_name))

    def write_to_file(self, content, file_path=None, mode='a'):
        with open(self.output_file, mode, encoding='utf-8') as f:
            if file_path:
                extension = os.path.splitext(file_path)[1]
                self.exported_files_count[extension] = self.exported_files_count.get(extension, 0) + 1
                lines = content.count('\n') + 1
                self.total_lines += lines
                
                # Store line count for this file
                self.line_counts_by_file[file_path] = lines
                
                # Add a note for converted ipynb files
                if extension == '.ipynb':
                    f.write(f"{self.delimiter}\nFull Path: {file_path}\n(NOTE: ipynb notebook converted to md)\n\n{content}\n\n")
                else:
                    f.write(f"{self.delimiter}\nFull Path: {file_path}\n\n{content}\n\n")
            else:
                f.write(f"{content}\n")

    def should_exclude_file(self, file_path):
        relative_path = os.path.relpath(file_path, self.repo_root)
        filename = os.path.basename(file_path)
        
        # First check blacklisted files
        if filename in self.blacklisted_files:
            return True
        
        # Then check always_exclude_patterns
        if any(filename.endswith(pattern) for pattern in self.always_exclude_patterns):
            return True
        
        # Finally check configured exclusions
        return any(relative_path.endswith(exclude) for exclude in self.files_to_exclude)

    def should_exclude_dir(self, dir_path):
        dir_name = os.path.basename(dir_path)
        
        # First check blacklisted dirs
        if dir_name in self.blacklisted_dirs:
            return True
        
        # Then check configured exclusions
        relative_path = os.path.relpath(dir_path, self.repo_root)
        relative_path = PathConverter.to_system_path(relative_path)
        return any(relative_path.startswith(PathConverter.to_system_path(exclude.rstrip('*'))) 
                  for exclude in self.subdirs_to_exclude)

    def traverse_directory(self, directory):
        # Ensure the directory path is absolute
        abs_directory = os.path.join(self.repo_root, directory)

        if not os.path.exists(abs_directory):
            print(f"Warning: Directory {abs_directory} does not exist. Skipping.")
            return

        for root, dirs, files in os.walk(abs_directory):
            dirs[:] = [d for d in dirs if not self.should_exclude_dir(os.path.join(root, d))]

            for file in files:
                file_path = os.path.join(root, file)
                if self.should_exclude_file(file_path):
                    continue
                file_extension = os.path.splitext(file)[1]
                if self.included_extensions == 'all' or file_extension in self.included_extensions:
                    relative_path = os.path.relpath(file_path, self.repo_root)
                    try:
                        with open(file_path, 'r', encoding='utf-8') as f:
                            content = f.read()
                        
                        # Convert ipynb to markdown if necessary
                        if file_extension == '.ipynb':
                            content = self.convert_ipynb_to_md(content)
                        
                        self.write_to_file(content, relative_path)
                    except Exception as e:
                        print(f"Error reading file {file_path}: {str(e)}")

    def include_specific_files(self, root_dir):
        """Include specific files, supporting both relative and absolute paths."""
        for file_path in self.files_to_include:
            if os.path.isabs(file_path):
                # Handle absolute paths directly
                if os.path.exists(file_path) and not self.should_exclude_file(file_path):
                    try:
                        with open(file_path, 'r', encoding='utf-8') as f:
                            content = f.read()
                        relative_path = os.path.relpath(file_path, self.repo_root)
                        self.write_to_file(content, relative_path)
                    except Exception as e:
                        print(f"Error reading file {file_path}: {str(e)}")
            else:
                # Original behavior for relative paths
                for root, _, files in os.walk(root_dir):
                    for file in files:
                        curr_path = os.path.join(root, file)
                        relative_path = os.path.relpath(curr_path, root_dir)
                        if relative_path == file_path and not self.should_exclude_file(curr_path):
                            try:
                                with open(curr_path, 'r', encoding='utf-8') as f:
                                    content = f.read()
                                self.write_to_file(content, relative_path)
                            except Exception as e:
                                print(f"Error reading file {curr_path}: {str(e)}")

    def should_include_in_tree(self, dir_path):
        """Determine if a directory should be included in the tree output."""
        dir_name = os.path.basename(dir_path)

        # Check blacklisted dirs first - make this an immediate return
        if dir_name in self.blacklisted_dirs:
            return False

        # Never include hidden directories (starting with .)
        if dir_name.startswith('.'):
            return False

        # If exhaustive_dir_tree is False, filter directories as they would be during traversal
        if not self.exhaustive_dir_tree:
            # Exclude directories that match subdirs_to_exclude
            if self.should_exclude_dir(dir_path):
                return False
            
            # Also ensure this directory is within one of our allowed traversal directories
            abs_dir_path = os.path.abspath(dir_path)
            allowed = False
            for d in self.dirs_to_traverse:
                allowed_dir = os.path.abspath(os.path.join(self.repo_root, d))
                if os.path.commonprefix([abs_dir_path, allowed_dir]) == allowed_dir:
                    allowed = True
                    break
            if not allowed:
                return False

        # If specific dirs are specified, only include those
        if self.dirs_for_tree:
            relative_path = os.path.relpath(dir_path, self.repo_root)
            return any(relative_path == d or relative_path.startswith(d + os.sep) 
                    for d in self.dirs_for_tree)

        return True

    def compute_directory_line_counts(self):
        """
        Compute the total line counts for each directory.
        We'll sum the line counts of all files under each directory.
        """
        self.line_counts_by_dir = {}

        # Initialize directories found from the exported files
        for rel_file_path, lines in self.line_counts_by_file.items():
            # Add line counts up the chain of directories
            parts = rel_file_path.split(os.sep)
            for i in range(1, len(parts)):
                dir_path = os.sep.join(parts[:i])  # partial path representing directory
                self.line_counts_by_dir[dir_path] = self.line_counts_by_dir.get(dir_path, 0) + lines

    def get_line_count_for_path(self, rel_path):
        """
        Return the line count for a given file or directory relative path.
        If it's a file, look up in line_counts_by_file.
        If it's a directory, look up in line_counts_by_dir.
        If not found, return 0.
        """
        if os.path.isfile(os.path.join(self.repo_root, rel_path)):
            return self.line_counts_by_file.get(rel_path, 0)
        else:
            return self.line_counts_by_dir.get(rel_path, 0)

    def get_directory_tree(self, directory, prefix='', current_depth=0, lines_word_used=False):
        """Generate a string representation of the directory tree with line counts."""
        if self.depth != -1 and current_depth > self.depth:
            return f"{prefix}│   └── (omitted)\n", lines_word_used

        tree_str = ''
        items = sorted(os.listdir(directory))
        
        # Filter items based on visibility rules
        visible_items = []
        for item in items:
            path = os.path.join(directory, item)
            if os.path.isfile(path):
                # Check if file was exported
                rel_file_path = os.path.relpath(path, self.repo_root)
                if rel_file_path in self.line_counts_by_file:  # only include if exported
                    visible_items.append(item)
            elif os.path.isdir(path):
                if self.should_include_in_tree(path):
                    visible_items.append(item)
        
        for i, item in enumerate(visible_items):
            path = os.path.join(directory, item)
            connector = '├── ' if i < len(visible_items) - 1 else '└── '
            rel_path = os.path.relpath(path, self.repo_root)
            line_count = self.get_line_count_for_path(rel_path)
            
            # Determine how to print line counts
            if not lines_word_used:
                # Print lines_word once
                line_str = f"({line_count} lines)" if line_count > 0 else "(0 lines)"
                lines_word_used = True
            else:
                line_str = f"({line_count})"
            
            tree_str += f"{prefix}{connector}{item} {line_str}\n"
            if os.path.isdir(path):
                extension = '' if i < len(visible_items) - 1 else '    '
                subtree_str, lines_word_used = self.get_directory_tree(path, prefix + extension + '│   ', current_depth + 1, lines_word_used)
                tree_str += subtree_str

        return tree_str, lines_word_used

    def export_repo(self):
        # Clear the output file before starting
        with open(self.output_file, 'w', encoding='utf-8') as f:
            pass

        # Write the export configuration to the output file if dump_config is True
        if self.dump_config:
            self.write_to_file(f"Export Configuration:\n{json.dumps(vars(self), indent=2)}", mode='w')

        # Export top-level files (if requested)
        if self.include_top_level_files == 'all':
            for item in os.listdir(self.repo_root):
                item_path = os.path.join(self.repo_root, item)
                if os.path.isfile(item_path):
                    # Add exclusion check before processing the file
                    if self.should_exclude_file(item_path):
                        continue
                    item_extension = os.path.splitext(item)[1]
                    if self.included_extensions == 'all' or item_extension in self.included_extensions:
                        with open(item_path, 'r', encoding='utf-8') as f:
                            content = f.read()
                        self.write_to_file(content, os.path.relpath(item_path, self.repo_root))
        elif isinstance(self.include_top_level_files, list):
            for file_name in self.include_top_level_files:
                if file_name in self.files_to_exclude:
                    continue
                file_path = os.path.join(self.repo_root, file_name)
                if os.path.exists(file_path):
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                    self.write_to_file(content, file_name)

        # Start traversal from each specified directory in dirs_to_traverse
        for dir in self.dirs_to_traverse:
            self.traverse_directory(dir)

        # Include specific files
        self.include_specific_files(self.repo_root)

        # Compute directory line counts
        self.compute_directory_line_counts()

        # Finally, write the directory tree with line counts
        directory_tree_str, _ = self.get_directory_tree(self.repo_root, current_depth=0, lines_word_used=False)
        header = f"Directory tree, stemming from root \"{self.repo_root}\":\n"
        # Overwrite the file with the directory tree at the top, then append exported files below
        # Actually, the user might prefer directory tree at the top. If so, we can prepend it:
        # Let's prepend it:
        with open(self.output_file, 'r', encoding='utf-8') as original:
            original_content = original.read()
        with open(self.output_file, 'w', encoding='utf-8') as modified:
            modified.write(header)
            modified.write(directory_tree_str)
            if original_content.strip():
                modified.write(self.delimiter + "\n" + original_content)

        # At the end of the script, after all processing
        print(f"Exported to: {self.output_file}")
        print(f"Total number of lines: {self.total_lines}")
        print("Number of exported files by extension:")
        for ext, count in self.exported_files_count.items():
            print(f"{ext}: {count}")

def get_base_path():
    """
    Determine the base path based on the host platform or command-line argument.
    """
    if '--pop' in sys.argv:
        return '/home/caleb/Documents/GitHub/'  # pop-xps popOS system
    elif platform.system() == "Darwin":  # macOS
        return "/Users/caleb/Documents/GitHub"
    elif platform.system() == "Windows":  # Windows
        return r"C:\Users\front\Documents\GitHub"
    else:  # Linux or other (dev server, or polliserve instances)
        return "/home/caleb/repo"

def load_config(config_filename):
    """
    Load configuration from a JSON file and adjust paths if necessary.
    """
    base_path = get_base_path()
    config_path = os.path.join(base_path, "utils/export_repo/configs", config_filename)
    config_path = PathConverter.to_system_path(config_path)
    
    with open(config_path, 'r', encoding='utf-8') as config_file:
        config = json.load(config_file)
    
    # Normalize all paths in config to system-specific format
    config = PathConverter.normalize_config_paths(config)
    return config

def get_default_config(repo_root):
    """
    Return a default configuration when no config file is provided.
    """
    return {
        'repo_root': repo_root,
        'export_name': f"{os.path.basename(repo_root)}_export.txt",
        'delimiter': '---',
        'dirs_to_traverse': ['.'],
        'include_top_level_files': 'all',
        'included_extensions': 'all',
        'subdirs_to_exclude': ['__pycache__'],
        'files_to_exclude': [],
        'depth': 10,
        'exhaustive_dir_tree': False,
        'files_to_include': [],
        'always_exclude_patterns': ['export.txt'],
        'dump_config': False
    }

def main():
    args = sys.argv[1:]
    config_filename = None
    pop_flag = False
    dump_config = False

    for arg in args:
        if arg == '--pop':
            pop_flag = True
        elif arg == '--dump-config':
            dump_config = True
        elif not arg.startswith('--'):
            config_filename = arg

    if not config_filename:
        print("Usage: python script.py [--pop] [--dump-config] <config_filename> or <repo_root>")
        sys.exit(1)

    if os.path.isdir(config_filename):
        # If the argument is a directory, use it as repo_root with default config
        config = get_default_config(config_filename)
    else:
        # If not a directory, treat it as a config file name
        if not config_filename.endswith('.json'):
            config_filename += '.json'
        config = load_config(config_filename)

    # Adjust repo_root based on pop_flag
    if pop_flag:
        base_path = '/home/caleb/Documents/GitHub/'
        config['repo_root'] = config['repo_root'].replace("/home/caleb/repo", base_path)

    # Set dump_config based on command-line flag
    config['dump_config'] = dump_config

    exporter = RepoExporter(config)
    exporter.export_repo()

if __name__ == "__main__":
    main()
</export_repo_to_txt.py>
<readme.md>
# Export Repository to Text File

## Overview

This tool exports repository content to a text file, including directory structures and file contents. It provides flexible configuration options to control what is included in the export.

## Usage

```bash
python export_repo_to_txt.py <config_file>.json [--dump-config]
```

or

```bash
python export_repo_to_txt.py <repo_root> [--dump-config]
```

## Configuration Parameters

- `repo_root`: Path to the repository's root directory.
- `export_name`: Name or path for the export file. If a path, exports there; if a name, exports to `repo_root`.
- `delimiter`: Separator string for entries in the export file.
- `dirs_to_traverse`: List of directories within `repo_root` for full traversal and export.
- `dirs_for_tree`: List of specific directories to include in the directory tree output. If empty, includes all non-hidden directories.
- `files_to_include`: List of specific files to include in the export, regardless of their location in the repository.
- `include_top_level_files`: Specifies top-level files for inclusion. Set to `"all"` for all files, or list specific files.
- `included_extensions`: File extensions to include. Use `"all"` for all extensions.
- `subdirs_to_exclude`: List of subdirectory names or paths to exclude from traversal.
- `files_to_exclude`: List of file names to exclude from the export.
- `always_exclude_patterns`: List of filename patterns to always exclude (e.g., ["export.txt"]).
- `depth`: Depth of directory traversal. `-1` for full traversal (default).
- `exhaustive_dir_tree`: If `true`, exports full directory tree regardless of other settings.
- `dump_config`: If `true`, dumps the export configuration JSON at the top of the output file.

## Example Configuration

```json
{
  "repo_root": "/home/user/myrepo",
  "export_name": "repo_export.txt",
  "delimiter": "----",
  "dirs_to_traverse": ["src", "docs"],
  "files_to_include": ["README.md", "config.json"],
  "include_top_level_files": "all",
  "included_extensions": [".py", ".md", ".json"],
  "subdirs_to_exclude": ["tests", "build"],
  "files_to_exclude": ["secrets.yaml"],
  "always_exclude_patterns": ["export.txt", "*.log"],
  "depth": -1,
  "exhaustive_dir_tree": false,
  "dump_config": false
}
```

## Notes

- The tool will automatically exclude the output file from the export.
- If no config file is provided, default settings will be used.
- The `subdirs_to_exclude` option supports partial paths (e.g., "foo/bar" will exclude all "bar" directories under any "foo" directory).
- Use `always_exclude_patterns` for files you want to exclude regardless of their location or other inclusion rules.
- To include the export configuration in the output file, use the `--dump-config` flag when running the script.
- Hidden directories (starting with '.') are automatically excluded from the directory tree.
- Use `dirs_for_tree` to explicitly specify which directories should appear in the directory tree output. If not specified, all non-hidden directories will be included.


## Example Configurations

### NextJS Project

```json
{
  "repo_root": "/home/user/nextjs-project",
  "export_name": "nextjs_project_export.txt",
  "delimiter": "----",
  "dirs_to_traverse": ["components", "pages", "styles", "public"],
  "dirs_for_tree": ["components", "pages", "styles"],  // Only show main app directories
  "include_top_level_files": ["package.json", "next.config.js"],
  "included_extensions": [".js", ".jsx", ".ts", ".tsx", ".css"]
}
```

### Python Project

```json
{
  "repo_root": "/home/user/python-project",
  "export_name": "python_project_export.txt",
  "delimiter": "----",
  "dirs_to_traverse": ["src", "tests", "docs"],
  "dirs_for_tree": ["src", "docs"],  // Only show source and documentation
  "files_to_include": ["requirements.txt", "setup.py"],
  "include_top_level_files": "all",
  "included_extensions": [".py", ".md", ".yml"],
  "subdirs_to_exclude": ["__pycache__", ".venv"]
}
```

## Setting up the `export_repo` alias

### On macOS

1. Open your terminal.

2. Open your shell configuration file (for zsh, it's usually `~/.zshrc`):
   ```bash
   nano ~/.zshrc
   ```

3. Add the following line at the end of the file:
   ```bash
   alias export_repo='/Users/caleb/Documents/GitHub/utils/.venv/bin/python /Users/caleb/Documents/GitHub/utils/export_repo/export_repo_to_txt.py'
   ```

4. Save the file and exit the editor (in nano, press Ctrl+X, then Y, then Enter).

5. Reload your shell configuration:
   ```bash
   source ~/.zshrc
   ```

### On PopOS (Linux)

1. Open your terminal.

2. Open your shell configuration file (for bash, it's usually `~/.bashrc`):
   ```bash
   nano ~/.bashrc
   ```

3. Add the following line at the end of the file:
   ```bash
   alias export_repo='/home/caleb/Documents/GitHub/utils/.venv/bin/python /home/caleb/Documents/GitHub/utils/export_repo/export_repo_to_txt.py --pop'
   ```

4. Save the file and exit the editor (in nano, press Ctrl+X, then Y, then Enter).

5. Reload your shell configuration:
   ```bash
   source ~/.bashrc
   ```

### On Windows (PowerShell)

1. Open PowerShell.

2. First, check if you already have a PowerShell profile:
   ```powershell
   Test-Path $PROFILE
   ```

3. If it returns False, create one:
   ```powershell
   New-Item -Path $PROFILE -Type File -Force
   ```

4. Open your PowerShell profile in a text editor:
   ```powershell
   notepad $PROFILE
   ```

5. Add this function to your profile:
   ```powershell
   function export_repo {
       & "C:\Users\front\Documents\GitHub\utils\.venv\Scripts\python.exe" "C:\Users\front\Documents\GitHub\utils\export_repo\export_repo_to_txt.py" $args
   }
   ```

6. Save the file and reload your profile:
   ```powershell
   . $PROFILE
   ```

Now you can use the `export_repo` command from anywhere in your terminal on macOS, PopOS, or Windows PowerShell. For example:

```bash
export_repo hFormer-codeOnly
# or 
export_repo /path/to/your/repo --dump-config
```

Notes: 
- On PopOS, the `--pop` flag is automatically included in the alias to ensure the correct base path is used
- On Windows, make sure you're using PowerShell and not Command Prompt (cmd.exe) as this alias will only work in PowerShell
</readme.md>
<dirtree>
Directory tree, stemming from root "/home/caleb/repo/utils/export_repo":
├── configs (455 lines)
│   ├── autocrop.json (10)
│   ├── bulk_dl.json (14)
│   ├── cosm-c360-tools.json (12)
│   ├── export_repo.json (10)
│   ├── ezmd.json (11)
│   ├── h5merge-mini.json (12)
│   ├── h5merge.json (12)
│   ├── h5pull.json (11)
│   ├── hFormer0-serve.json (10)
│   ├── ibrida.json (12)
│   ├── ibridaDB_v0r1_export.json (12)
│   ├── ibridaDB_v0rX.json (12)
│   ├── ibridaV2_generator.json (15)
│   ├── ladybird_data.json (13)
│   ├── metaformer.json (11)
│   ├── metaformer1.json (12)
│   ├── metaformer2.json (11)
│   ├── model-explorer.json (13)
│   ├── nextjs.json (8)
│   ├── polliFormer-COPAP.json (12)
│   ├── polliFormer-Dyn.json (12)
│   ├── polliFormer-blade-angio-0.json (14)
│   ├── polliFormer-buildData.json (15)
│   ├── polliFormer-codeOnly.json (11)
│   ├── polliFormer-configModel.json (14)
│   ├── polliFormer-h5Data.json (13)
│   ├── polliFormer-models-codeOnly.json (11)
│   ├── polliFormer-models.json (11)
│   ├── polliFormer-modelsDyn.json (11)
│   ├── polliFormer-modelsPruned-codeOnly.json (11)
│   ├── polliFormer-modelsPrunedInv-codeOnly.json (11)
│   ├── polliFormer-serve.json (10)
│   ├── polliFormer-tests.json (12)
│   ├── polliFormer.json (13)
│   ├── polliOS-codeOnly.json (13)
│   ├── polliOS.json (13)
│   ├── sam2.json (12)
│   └── sam2_demo.json (15)
└── export_repo_to_txt.py (453)
</dirtree
</codebase>

<bug>
When exporting the ezmd repo with the following config:
```
{
    "repo_root": "/home/caleb/repo/ezmd",
    "export_name": "ezmd_export.txt",
    "delimiter": "----",
    "dirs_to_traverse": ["src", "tests"],
    "subdirs_to_exclude": ["dev"],
    "include_top_level_files": "all",
    "included_extensions": [".py", ".env", ".toml", ".md"],
    "always_exclude_patterns": ["uv.lock", "export.txt", ".log", ".venv", ".gitignore"],
    "exhaustive_dir_tree": false
  }
```
The file '/home/caleb/repo/ezmd/dev/prompts.txt' is incorrectly included in the export. That file should not be included in the export, and should have been caught by several different rules.
</bug>
<discussion>
So, given this dirtree in the target repo:

```
Directory tree, stemming from root "ezmd":
├── .gitignore (172 lines)
├── .python-version (2)
├── README.md (1)
├── dev (73)
│   └── prompts.txt (73)
├── pyproject.toml (12)
├── src (0)
└── tests (0)
```
And the disinclusion of the 'dev' subdir from the dirs_to_traverse list, the inclusion of 'dev' in the subdirs_to_exclude list, and the disinclusion of '.txt' in the "included_extensions" list, there are zero reasons why the prompts.txt file should be included in the export, and several reasons why prompts.txt should have been excluded.
</discussion>
<context>
This is a hodgepodge personal tool that I use to gather context to prompt LLMs. Because I have added additional rules as-needed to get a particular outcome without considering the soundness of the overall ruleset or logical flows, the system is a patchwork, and I'm not surprised that we're encountering this conflict. To reduce the number of headaches that this tool causes me in the future, we should probably design the ruleset de-novo to simplify the logical chains and make the tool easier to use. The reformulated tool should achieve full coverage of the existing feature set, but should have fewer configurable parameters, each with a well-defined scope. My typical usage pattern for this tool is to define a general set of inclusion rules, and then to prune with finer-grained exclusion rules (like exclude_subdir, exclude_file), and to sometimes pull in files outside the scope of the root dir by providing explicit paths.

I am mostly satisfied with the formatting of the export txtfiles generated by this tool (including the dirtree and the file-by-file dumps), but we should make the outputs a bit more LLM-friendly by structuring the outputs with XML tags (instead of the delimiters that we currently use, which are tailored for human eyes). We should wrap the entire output file with a <codebase></codebase> tag. Then, use XML delimeters to separate into <directory_tree> and the <files> sections. Finally, separate each file in the file-by-file dumps (i.e. the codebase_files section) with XML tags-- use the filename as the tag. Let's also use XML tags to group the file-by-file dumps by their directory hierarchy, ex. in this codebase <export_repo.py> would be nested under <export_repo>, which would itself be nested under <files>. Remember that both top-level sections, <files> and <directory_tree> would be nested under the <codebase> tag-- <codebase> wraps the entire export file so that we can paste the output directly into our LLM prompt (I use highly-structured prompts, where each component of the prompt is XML-structured-- you can see an example of this in this very message.)

Note that we may add an interactive mode in the future; the usage pattern here would be to have a general config per-repo but to allow the user to quickly compose tailored exports for different tasks within a TUI.. in most cases, this would be quicker and more maintainable than writing a new config file each time, which can quickly grow to an overwhelming scale. If we implement this feature, we'd use a sidecar file to cache the 10-20 most recently used 'tailored' prompts (globally, and for each of the base per-repo configs) for quick reuse. We would also add a tool version tracking system to the configs/sidecar files to help us deal with breaking changes in the core logic and rulesets. So design the new ruleset and architectures with this future requirement in mind-- not sure if we'll do this today, but it's been on my todo list for a while. 

I've had a few folks on twitter ask to use my tool, but it's grown into such a mess over time that I haven't published it. If I'm satisfied with what we come up with today, I may finally publish this tool-- and I'll throw you some credit! So let's take all the time we need to think deeply and come up with something great.
</context>
<task_0>
Fully trace the ruleset application flow for this ezmd export config. Identify the root cause of the logical flaw that led to the incorrect inclusion of ezmd/dev/prompts.txt in the export.
</task_0>
<task_1>
Exhaustively map the ruleset hierarchy. Identify conflicting rules. Identify rules with overlapping scopes (there are many).
</task_1>
<task_2>
Propose a 'smarter' ruleset that we can use in a reimplementation of this tool. The user should be able to achieve the same level of granularity/control over the export, but the median export config json should be substantially simpler. The user shouldn't need to consult the README every time they need to draft a new export config-- the parameters should have clearly-defined scopes, with descriptive names and a self-evident relational structure.

Think through every aspect of the new system. We'll focus on the core logical structure for now-- let's dial in a production-grade ruleset that optimizes for consistency, simplicity, smart defaults, and granularity.
</task_2>
<task_3>
Review the proposed interactive export_repo tool that I described earlier-- just so we don't forgot the rough guidelines I described in the context section, please summarize and structure the first-draft feature set that I described. While this new feature isn't our focus in this message, I want to keep track of these initial thoughts I had in case we decide to move forward with this feature tonight.

Provide some follow-up questions to clarify the high-level requirements that I'd be looking for in this feature. When we're ready to work on this feature, I'll respond to these and we can iteratively architect a well-considered interactive mode; in the long run I think this might be the most common means by which users interact with our tool. I've been particularly inspired by the 'Repo Prompt' tool that has recently launched (as a dev preview) on TestFlight-- twitter seems to love it, and so I'm inspired to revisit this idea.
</task_3>
<think_slowly_and_deeply>
Take as much time as you need to think about these tasks. This is a pretty fun problem-- identifying the scope and deducing the logical chains in our old tool's rulesets, identifying the key filtering requirements, and then taking the opportunity to conceptualize an optimized ruleset algorithm. Since we're essentially starting from fresh, this is a rare opportunity to design an algorithm from scratch (albeit within the constraints that we need to preserve the ability to configure exports at a granularity at least as fine as the old tool). This is about as close to a pure logics puzzle as it gets-- so let's take our time and think into deep conceptual space, and we can bring things back to our actual software implementation later.
</<think_slowly_and_deeply>
---
</file: new_export_repo_prompts.xml>

<file: readme.md>
# Export Repository to Text File

## Overview

This tool exports repository content to a text file, including directory structures and file contents. It provides flexible configuration options to control what is included in the export.

## Usage

```bash
python export_repo_to_txt.py <config_file>.json [--dump-config]
```

or

```bash
python export_repo_to_txt.py <repo_root> [--dump-config]
```

## Configuration Parameters

- `repo_root`: Path to the repository's root directory.
- `export_name`: Name or path for the export file. If a path, exports there; if a name, exports to `repo_root`.
- `delimiter`: Separator string for entries in the export file.
- `dirs_to_traverse`: List of directories within `repo_root` for full traversal and export.
- `dirs_for_tree`: List of specific directories to include in the directory tree output. If empty, includes all non-hidden directories.
- `files_to_include`: List of specific files to include in the export, regardless of their location in the repository.
- `include_top_level_files`: Specifies top-level files for inclusion. Set to `"all"` for all files, or list specific files.
- `included_extensions`: File extensions to include. Use `"all"` for all extensions.
- `subdirs_to_exclude`: List of subdirectory names or paths to exclude from traversal.
- `files_to_exclude`: List of file names to exclude from the export.
- `always_exclude_patterns`: List of filename patterns to always exclude (e.g., ["export.txt"]).
- `depth`: Depth of directory traversal. `-1` for full traversal (default).
- `exhaustive_dir_tree`: If `true`, exports full directory tree regardless of other settings.
- `dump_config`: If `true`, dumps the export configuration JSON at the top of the output file.

## Example Configuration

```json
{
  "repo_root": "/home/user/myrepo",
  "export_name": "repo_export.txt",
  "delimiter": "----",
  "dirs_to_traverse": ["src", "docs"],
  "files_to_include": ["README.md", "config.json"],
  "include_top_level_files": "all",
  "included_extensions": [".py", ".md", ".json"],
  "subdirs_to_exclude": ["tests", "build"],
  "files_to_exclude": ["secrets.yaml"],
  "always_exclude_patterns": ["export.txt", "*.log"],
  "depth": -1,
  "exhaustive_dir_tree": false,
  "dump_config": false
}
```

## Notes

- The tool will automatically exclude the output file from the export.
- If no config file is provided, default settings will be used.
- The `subdirs_to_exclude` option supports partial paths (e.g., "foo/bar" will exclude all "bar" directories under any "foo" directory).
- Use `always_exclude_patterns` for files you want to exclude regardless of their location or other inclusion rules.
- To include the export configuration in the output file, use the `--dump-config` flag when running the script.
- Hidden directories (starting with '.') are automatically excluded from the directory tree.
- Use `dirs_for_tree` to explicitly specify which directories should appear in the directory tree output. If not specified, all non-hidden directories will be included.


## Example Configurations

### NextJS Project

```json
{
  "repo_root": "/home/user/nextjs-project",
  "export_name": "nextjs_project_export.txt",
  "delimiter": "----",
  "dirs_to_traverse": ["components", "pages", "styles", "public"],
  "dirs_for_tree": ["components", "pages", "styles"],  // Only show main app directories
  "include_top_level_files": ["package.json", "next.config.js"],
  "included_extensions": [".js", ".jsx", ".ts", ".tsx", ".css"]
}
```

### Python Project

```json
{
  "repo_root": "/home/user/python-project",
  "export_name": "python_project_export.txt",
  "delimiter": "----",
  "dirs_to_traverse": ["src", "tests", "docs"],
  "dirs_for_tree": ["src", "docs"],  // Only show source and documentation
  "files_to_include": ["requirements.txt", "setup.py"],
  "include_top_level_files": "all",
  "included_extensions": [".py", ".md", ".yml"],
  "subdirs_to_exclude": ["__pycache__", ".venv"]
}
```

## Setting up the `export_repo` alias

### On macOS

1. Open your terminal.

2. Open your shell configuration file (for zsh, it's usually `~/.zshrc`):
   ```bash
   nano ~/.zshrc
   ```

3. Add the following line at the end of the file:
   ```bash
   alias export_repo='/Users/caleb/Documents/GitHub/utils/.venv/bin/python /Users/caleb/Documents/GitHub/utils/export_repo/export_repo_to_txt.py'
   ```

4. Save the file and exit the editor (in nano, press Ctrl+X, then Y, then Enter).

5. Reload your shell configuration:
   ```bash
   source ~/.zshrc
   ```

### On PopOS (Linux)

1. Open your terminal.

2. Open your shell configuration file (for bash, it's usually `~/.bashrc`):
   ```bash
   nano ~/.bashrc
   ```

3. Add the following line at the end of the file:
   ```bash
   alias export_repo='/home/caleb/Documents/GitHub/utils/.venv/bin/python /home/caleb/Documents/GitHub/utils/export_repo/export_repo_to_txt.py --pop'
   ```

4. Save the file and exit the editor (in nano, press Ctrl+X, then Y, then Enter).

5. Reload your shell configuration:
   ```bash
   source ~/.bashrc
   ```

### On Windows (PowerShell)

1. Open PowerShell.

2. First, check if you already have a PowerShell profile:
   ```powershell
   Test-Path $PROFILE
   ```

3. If it returns False, create one:
   ```powershell
   New-Item -Path $PROFILE -Type File -Force
   ```

4. Open your PowerShell profile in a text editor:
   ```powershell
   notepad $PROFILE
   ```

5. Add this function to your profile:
   ```powershell
   function export_repo {
       & "C:\Users\front\Documents\GitHub\utils\.venv\Scripts\python.exe" "C:\Users\front\Documents\GitHub\utils\export_repo\export_repo_to_txt.py" $args
   }
   ```

6. Save the file and reload your profile:
   ```powershell
   . $PROFILE
   ```

Now you can use the `export_repo` command from anywhere in your terminal on macOS, PopOS, or Windows PowerShell. For example:

```bash
export_repo hFormer-codeOnly
# or 
export_repo /path/to/your/repo --dump-config
```

Notes: 
- On PopOS, the `--pop` flag is automatically included in the alias to ensure the correct base path is used
- On Windows, make sure you're using PowerShell and not Command Prompt (cmd.exe) as this alias will only work in PowerShell
</file: readme.md>

<file: export_repo_to_txt.py>
import platform
import os
import json
import sys
import nbformat
from nbconvert import MarkdownExporter
from nbconvert.preprocessors import ClearOutputPreprocessor

class PathConverter:
    @staticmethod
    def to_system_path(path: str) -> str:
        """
        Convert the given path to the current system's native format.
        On Windows, forward slashes become backslashes, etc.
        """
        if platform.system() == "Windows":
            # Convert forward slashes to backslashes and handle drive letter
            if path.startswith("/"):
                # Remove leading slash and convert to Windows path
                path = path.lstrip("/")
                if ":" not in path:  # If no drive letter, assume C:
                    path = "C:\\" + path
            return path.replace("/", "\\")
        else:
            # Convert backslashes to forward slashes for Unix-like systems
            return path.replace("\\", "/")

    @staticmethod
    def normalize_config_paths(config: dict) -> dict:
        """
        Normalize all relevant paths in the config to the current system's format.
        """
        if 'repo_root' in config:
            base_path = get_base_path()
            if platform.system() == "Windows":
                # Replace Unix-style base paths with Windows GitHub path
                for unix_path in ["/home/caleb/repo", "/home/caleb/Documents/GitHub/", "/Users/caleb/Documents/GitHub"]:
                    if config['repo_root'].startswith(unix_path):
                        relative_path = config['repo_root'][len(unix_path):].lstrip("/")
                        config['repo_root'] = os.path.join(base_path, relative_path)
                        break
            
            config['repo_root'] = PathConverter.to_system_path(config['repo_root'])
        
        # Convert path lists
        for key in ['dirs_to_traverse', 'subdirs_to_exclude', 'files_to_exclude']:
            if key in config and isinstance(config[key], list):
                config[key] = [PathConverter.to_system_path(p) for p in config[key]]
        
        return config

class RepoExporter:
    def __init__(self, config: dict, config_filename: str = None):
        """
        Initialize the RepoExporter with the given config dictionary.
        :param config: The loaded or constructed configuration object.
        :param config_filename: Optional name of the config file (if used), 
                               for placing in the output XML tags.
        """
        # Convert all paths in config to system-specific format
        config = PathConverter.normalize_config_paths(config)
        self.repo_root = config['repo_root']
        self.export_name = config['export_name']
        self.dirs_to_traverse = config['dirs_to_traverse']
        self.include_top_level_files = config['include_top_level_files']
        self.included_extensions = config['included_extensions']
        self.subdirs_to_exclude = config.get('subdirs_to_exclude', [])
        self.files_to_exclude = config.get('files_to_exclude', [])
        self.depth = config.get('depth', -1)  # Default to -1 for full traversal
        self.dump_config = config.get('dump_config', False)
        self.exhaustive_dir_tree = config.get('exhaustive_dir_tree', False)

        # Additional rules and blacklists
        self.blacklisted_dirs = ['__pycache__', '.git', '.venv', '.vscode']
        self.blacklisted_files = ['uv.lock', 'LICENSE']
        self.files_to_include = config.get('files_to_include', [])
        self.always_exclude_patterns = config.get('always_exclude_patterns', ['export.txt'])
        self.dirs_for_tree = config.get('dirs_for_tree', [])

        # Bookkeeping
        self.config_filename = config_filename
        self.exported_files_count = {}
        self.total_lines = 0
        self.line_counts_by_file = {}
        self.line_counts_by_dir = {}
        
        # Make sure the final output path is correct
        self.output_file = self.get_output_file_path()
        # Avoid re-exporting the output file
        self.files_to_exclude.append(os.path.basename(self.output_file))

        # Memory buffer for file contents (rather than writing them incrementally)
        # Each entry is a tuple: (rel_path, content, is_ipynb_converted)
        self.file_contents = []

    def get_output_file_path(self) -> str:
        """
        Return the absolute path for the export file, 
        converting paths if necessary.
        """
        if os.path.isabs(self.export_name):
            return PathConverter.to_system_path(self.export_name)
        else:
            return PathConverter.to_system_path(os.path.join(self.repo_root, self.export_name))

    def convert_ipynb_to_md(self, notebook_content: str) -> str:
        """
        Convert an IPython notebook JSON string to Markdown by clearing outputs
        and using nbconvert's MarkdownExporter.
        """
        notebook = nbformat.reads(notebook_content, as_version=4)
        
        # Clear outputs
        clear_output = ClearOutputPreprocessor()
        clear_output.preprocess(notebook, {})

        # Convert to markdown
        markdown_exporter = MarkdownExporter()
        markdown_content, _ = markdown_exporter.from_notebook_node(notebook)
        return markdown_content

    def store_file_content(self, content: str, file_path: str, ipynb_converted: bool = False):
        """
        Cache the file's content in memory, track line counts, 
        and update the total lines + extension stats.
        """
        extension = os.path.splitext(file_path)[1]
        self.exported_files_count[extension] = self.exported_files_count.get(extension, 0) + 1
        
        line_count = content.count('\n') + 1
        self.total_lines += line_count
        self.line_counts_by_file[file_path] = line_count

        # Store the file content in memory for final output
        self.file_contents.append((file_path, content, ipynb_converted))

    def should_exclude_file(self, file_path: str) -> bool:
        """
        Return True if the file_path should be skipped 
        based on blacklists, always_exclude_patterns, and user config.
        """
        relative_path = os.path.relpath(file_path, self.repo_root)
        filename = os.path.basename(file_path)
        
        # Check blacklisted files
        if filename in self.blacklisted_files:
            return True
        
        # Patterns
        if any(filename.endswith(pattern) for pattern in self.always_exclude_patterns):
            return True
        
        # Config-based exclude
        return any(relative_path.endswith(exclude) for exclude in self.files_to_exclude)

    def should_exclude_dir(self, dir_path: str) -> bool:
        """
        Return True if the directory should be skipped 
        based on blacklists and user config.
        """
        dir_name = os.path.basename(dir_path)

        # Blacklisted names
        if dir_name in self.blacklisted_dirs:
            return True
        
        # Config-based excludes
        relative_path = os.path.relpath(dir_path, self.repo_root)
        relative_path = PathConverter.to_system_path(relative_path)
        return any(relative_path.startswith(PathConverter.to_system_path(exclude.rstrip('*'))) 
                   for exclude in self.subdirs_to_exclude)

    def traverse_directory(self, directory: str):
        """
        Walk through 'directory' within the repo_root, 
        read all matching files, and store their contents in memory.
        """
        abs_directory = os.path.join(self.repo_root, directory)

        if not os.path.exists(abs_directory):
            print(f"Warning: Directory {abs_directory} does not exist. Skipping.")
            return

        for root, dirs, files in os.walk(abs_directory):
            # Filter out excluded subdirs
            dirs[:] = [d for d in dirs if not self.should_exclude_dir(os.path.join(root, d))]

            # Check each file
            for file in files:
                file_path = os.path.join(root, file)
                if self.should_exclude_file(file_path):
                    continue
                file_extension = os.path.splitext(file)[1]
                # Check if extension is included
                if self.included_extensions == 'all' or file_extension in self.included_extensions:
                    relative_path = os.path.relpath(file_path, self.repo_root)
                    try:
                        with open(file_path, 'r', encoding='utf-8') as f:
                            content = f.read()
                        # Convert .ipynb to .md if necessary
                        if file_extension == '.ipynb':
                            content = self.convert_ipynb_to_md(content)
                            self.store_file_content(content, relative_path, ipynb_converted=True)
                        else:
                            self.store_file_content(content, relative_path, ipynb_converted=False)
                    except Exception as e:
                        print(f"Error reading file {file_path}: {str(e)}")

    def include_specific_files(self):
        """
        Include user-specified files in self.files_to_include, 
        supporting absolute or relative paths.
        """
        for file_path in self.files_to_include:
            # Check absolute vs. relative
            if os.path.isabs(file_path):
                # Handle absolute path directly
                if os.path.exists(file_path) and not self.should_exclude_file(file_path):
                    try:
                        with open(file_path, 'r', encoding='utf-8') as f:
                            content = f.read()
                        relative_path = os.path.relpath(file_path, self.repo_root)
                        extension = os.path.splitext(file_path)[1]
                        if extension == '.ipynb':
                            content = self.convert_ipynb_to_md(content)
                            self.store_file_content(content, relative_path, ipynb_converted=True)
                        else:
                            self.store_file_content(content, relative_path, ipynb_converted=False)
                    except Exception as e:
                        print(f"Error reading file {file_path}: {str(e)}")
            else:
                # It's a relative path from repo_root
                abs_root = os.path.abspath(self.repo_root)
                for root, _, files in os.walk(abs_root):
                    for file in files:
                        curr_path = os.path.join(root, file)
                        rel_path = os.path.relpath(curr_path, abs_root)
                        if rel_path == file_path and not self.should_exclude_file(curr_path):
                            try:
                                with open(curr_path, 'r', encoding='utf-8') as f:
                                    content = f.read()
                                extension = os.path.splitext(curr_path)[1]
                                if extension == '.ipynb':
                                    content = self.convert_ipynb_to_md(content)
                                    self.store_file_content(content, file_path, ipynb_converted=True)
                                else:
                                    self.store_file_content(content, file_path, ipynb_converted=False)
                            except Exception as e:
                                print(f"Error reading file {curr_path}: {str(e)}")

    def should_include_in_tree(self, dir_path: str) -> bool:
        """
        Return True if 'dir_path' should appear in the directory tree.
        This logic respects blacklists, user excludes, and (optionally) self.exhaustive_dir_tree.
        """
        dir_name = os.path.basename(dir_path)

        # Immediately drop blacklisted or hidden
        if dir_name in self.blacklisted_dirs or dir_name.startswith('.'):
            return False

        # If exhaustive_dir_tree is off, filter further
        if not self.exhaustive_dir_tree:
            if self.should_exclude_dir(dir_path):
                return False
            # Also ensure directory is within dirs_to_traverse
            abs_dir_path = os.path.abspath(dir_path)
            allowed = False
            for d in self.dirs_to_traverse:
                allowed_dir = os.path.abspath(os.path.join(self.repo_root, d))
                if os.path.commonprefix([abs_dir_path, allowed_dir]) == allowed_dir:
                    allowed = True
                    break
            if not allowed:
                return False

        # If dirs_for_tree is specified, only include matches
        if self.dirs_for_tree:
            relative_path = os.path.relpath(dir_path, self.repo_root)
            return any(
                relative_path == d or relative_path.startswith(d + os.sep)
                for d in self.dirs_for_tree
            )

        return True

    def compute_directory_line_counts(self):
        """
        After reading all file contents, sum up the line counts
        for each directory that contains them.
        """
        self.line_counts_by_dir = {}
        for rel_file_path, lines in self.line_counts_by_file.items():
            parts = rel_file_path.split(os.sep)
            # Add line counts up the chain of directories
            for i in range(1, len(parts)):
                dir_path = os.sep.join(parts[:i])
                self.line_counts_by_dir[dir_path] = self.line_counts_by_dir.get(dir_path, 0) + lines

    def get_line_count_for_path(self, rel_path: str) -> int:
        """
        Return the line count for the given rel_path 
        (if file) or sum for all files under that directory.
        """
        full_path = os.path.join(self.repo_root, rel_path)
        if os.path.isfile(full_path):
            return self.line_counts_by_file.get(rel_path, 0)
        else:
            return self.line_counts_by_dir.get(rel_path, 0)

    def get_directory_tree(self, directory: str, prefix: str = '', current_depth: int = 0, lines_word_used: bool = False):
        """
        Produce a string representation of 'directory' in an ASCII tree format, 
        labeling each item with either "(X lines)" or "(X)" (once "lines" has appeared).
        """
        if self.depth != -1 and current_depth > self.depth:
            return f"{prefix}   (omitted)\n", lines_word_used

        tree_str = ''
        items = sorted(os.listdir(directory))
        visible_items = []
        for item in items:
            path = os.path.join(directory, item)
            if os.path.isfile(path):
                # Show file if it was actually exported
                rel_file_path = os.path.relpath(path, self.repo_root)
                if rel_file_path in self.line_counts_by_file:
                    visible_items.append(item)
            elif os.path.isdir(path):
                if self.should_include_in_tree(path):
                    visible_items.append(item)

        for i, item in enumerate(visible_items):
            path = os.path.join(directory, item)
            rel_path = os.path.relpath(path, self.repo_root)
            line_count = self.get_line_count_for_path(rel_path)

            # Use ASCII connectors
            connector = '|-- ' if i < len(visible_items) - 1 else '\\-- '

            if not lines_word_used:
                # first time we show line counts, add the word "lines"
                line_str = f"({line_count} lines)" if line_count > 0 else "(0 lines)"
                lines_word_used = True
            else:
                line_str = f"({line_count})"

            tree_str += f"{prefix}{connector}{item} {line_str}\n"
            if os.path.isdir(path):
                # Subtree indentation: align with the connector
                sub_prefix = prefix + ("|   " if i < len(visible_items) - 1 else "    ")
                subtree_str, lines_word_used = self.get_directory_tree(path, sub_prefix, current_depth + 1, lines_word_used)
                tree_str += subtree_str

        return tree_str, lines_word_used

    def export_repo(self):
        """
        Main routine:
        1) Possibly gather top-level files,
        2) Recursively gather all files from dirs_to_traverse,
        3) Include any extra user-specified files,
        4) Compute line counts,
        5) Write a final output file that uses XML-style tags:
           <codebase_context> 
              <repo export config: ...> ... </repo export config: ...>
              <dirtree: ...> ... </dirtree: ...>
              <file: ...> ... </file: ...>
           </codebase_context>
        """
        # 1) Optionally include top-level files
        if self.include_top_level_files == 'all':
            for item in os.listdir(self.repo_root):
                item_path = os.path.join(self.repo_root, item)
                if os.path.isfile(item_path) and not self.should_exclude_file(item_path):
                    file_extension = os.path.splitext(item)[1]
                    if self.included_extensions == 'all' or file_extension in self.included_extensions:
                        try:
                            with open(item_path, 'r', encoding='utf-8') as f:
                                content = f.read()
                            if file_extension == '.ipynb':
                                content = self.convert_ipynb_to_md(content)
                                self.store_file_content(content, os.path.relpath(item_path, self.repo_root), True)
                            else:
                                self.store_file_content(content, os.path.relpath(item_path, self.repo_root), False)
                        except Exception as e:
                            print(f"Error reading top-level file {item_path}: {str(e)}")
        elif isinstance(self.include_top_level_files, list):
            for file_name in self.include_top_level_files:
                if file_name in self.files_to_exclude:
                    continue
                file_path = os.path.join(self.repo_root, file_name)
                if os.path.exists(file_path) and not self.should_exclude_file(file_path):
                    try:
                        with open(file_path, 'r', encoding='utf-8') as f:
                            content = f.read()
                        extension = os.path.splitext(file_name)[1]
                        if extension == '.ipynb':
                            content = self.convert_ipynb_to_md(content)
                            self.store_file_content(content, file_name, True)
                        else:
                            self.store_file_content(content, file_name, False)
                    except Exception as e:
                        print(f"Error reading file {file_path}: {str(e)}")

        # 2) Traverse specified directories
        for dir_to_traverse in self.dirs_to_traverse:
            self.traverse_directory(dir_to_traverse)

        # 3) Include specific user-specified files
        self.include_specific_files()

        # 4) Compute directory line counts
        self.compute_directory_line_counts()

        # 5) Build the directory tree string
        directory_tree_str, _ = self.get_directory_tree(self.repo_root, current_depth=0, lines_word_used=False)

        # 6) Now write the final output with XML-style tags
        with open(self.output_file, 'w', encoding='utf-8') as out:
            out.write("<codebase_context>\n\n")

            # (optional) export config block
            if self.dump_config:
                # We'll label with the config filename if it exists
                # or just 'inline-config' if it doesn't
                config_tag_label = self.config_filename if self.config_filename else "inline-config"
                out.write(f"<repo export config: {config_tag_label}>\n")
                # For debugging, we can dump a JSON version of the object's fields.
                # Typically you'd store something more minimal. 
                # We'll replicate the older logic:
                config_data = {
                    # self.* variables that might be relevant to see
                    "repo_root": self.repo_root,
                    "export_name": self.export_name,
                    "dirs_to_traverse": self.dirs_to_traverse,
                    "include_top_level_files": self.include_top_level_files,
                    "included_extensions": self.included_extensions,
                    "subdirs_to_exclude": self.subdirs_to_exclude,
                    "files_to_exclude": self.files_to_exclude,
                    "depth": self.depth,
                    "dump_config": self.dump_config,
                    "exhaustive_dir_tree": self.exhaustive_dir_tree,
                    "blacklisted_dirs": self.blacklisted_dirs,
                    "blacklisted_files": self.blacklisted_files,
                    "files_to_include": self.files_to_include,
                    "always_exclude_patterns": self.always_exclude_patterns,
                    "dirs_for_tree": self.dirs_for_tree
                }
                out.write(json.dumps(config_data, indent=2))
                out.write(f"\n</repo export config: {config_tag_label}>\n\n")

            # Directory tree block
            out.write(f"<dirtree: {self.repo_root}>\n")
            out.write(directory_tree_str)
            out.write(f"</dirtree: {self.repo_root}>\n\n")

            # Files
            for (rel_path, content, ipynb_converted) in self.file_contents:
                out.write(f"<file: {rel_path}>\n")
                if ipynb_converted:
                    out.write("(NOTE: ipynb notebook converted to md)\n")
                out.write(content.rstrip('\n'))
                out.write(f"\n</file: {rel_path}>\n\n")

            out.write("</codebase_context>\n")

        # Console summary
        print(f"Exported to: {self.output_file}")
        print(f"Total number of lines: {self.total_lines}")
        print("Number of exported files by extension:")
        for ext, count in self.exported_files_count.items():
            print(f"  {ext}: {count}")

def get_base_path() -> str:
    """
    Determine the base path based on the host platform or command-line argument.
    """
    if '--pop' in sys.argv:
        return '/home/caleb/Documents/GitHub/'  # pop-xps popOS system
    elif platform.system() == "Darwin":  # macOS
        return "/Users/caleb/Documents/GitHub"
    elif platform.system() == "Windows":  # Windows
        return r"C:\Users\front\Documents\GitHub"
    else:  # Linux or other (dev server, or polliserve instances)
        return "/home/caleb/repo"

def load_config(config_filename: str) -> dict:
    """
    Load configuration from a JSON file located in `utils/export_repo/configs`,
    normalizing all paths inside.
    """
    base_path = get_base_path()
    config_path = os.path.join(base_path, "utils/export_repo/configs", config_filename)
    config_path = PathConverter.to_system_path(config_path)
    
    with open(config_path, 'r', encoding='utf-8') as config_file:
        config = json.load(config_file)
    
    # Normalize
    config = PathConverter.normalize_config_paths(config)
    return config

def get_default_config(repo_root: str) -> dict:
    """
    Provide a default config if the user passes a directory path 
    instead of a config file.
    """
    return {
        'repo_root': repo_root,
        'export_name': f"{os.path.basename(repo_root)}_export.txt",
        'dirs_to_traverse': ['.'],
        'include_top_level_files': 'all',
        'included_extensions': 'all',
        'subdirs_to_exclude': ['__pycache__'],
        'files_to_exclude': [],
        'depth': 10,
        'exhaustive_dir_tree': False,
        'files_to_include': [],
        'always_exclude_patterns': ['export.txt'],
        'dump_config': False
    }

def main():
    """
    Command-line entry point.
    Usage:
      python export_repo_to_txt.py [--pop] [--dump-config] <config_filename|repo_root>
    """
    args = sys.argv[1:]
    config_filename = None
    pop_flag = False
    dump_config_flag = False

    for arg in args:
        if arg == '--pop':
            pop_flag = True
        elif arg == '--dump-config':
            dump_config_flag = True
        elif not arg.startswith('--'):
            config_filename = arg

    if not config_filename:
        print("Usage: python export_repo_to_txt.py [--pop] [--dump-config] <config_filename or repo_root>")
        sys.exit(1)

    # Distinguish whether the user gave us a directory or a config file
    if os.path.isdir(config_filename):
        # Use a default config if a directory was passed
        config = get_default_config(config_filename)
        # We'll embed the actual path in config_filename for labeling if we want
        config_filename_label = f"default-for-{os.path.basename(config_filename)}"
    else:
        # If not a directory, treat it as a config file name
        if not config_filename.endswith('.json'):
            config_filename += '.json'
        config = load_config(config_filename)
        config_filename_label = config_filename

    # If pop_flag was set, do any path transformations
    if pop_flag:
        base_path = '/home/caleb/Documents/GitHub/'
        config['repo_root'] = config['repo_root'].replace("/home/caleb/repo", base_path)

    # If --dump-config was requested, set in config
    config['dump_config'] = dump_config_flag

    # Create and run exporter
    exporter = RepoExporter(config, config_filename=config_filename_label)
    exporter.export_repo()

if __name__ == "__main__":
    main()
</file: export_repo_to_txt.py>

<file: new_export_repo_prompts.xml>
<codebase>
<export_repo_to_txt.py>
import platform
import os
import json
import sys
import nbformat
from nbconvert import MarkdownExporter
from nbconvert.preprocessors import ClearOutputPreprocessor

class PathConverter:
    @staticmethod
    def to_system_path(path):
        """Convert path to the current system's format."""
        if platform.system() == "Windows":
            # Convert forward slashes to backslashes and handle drive letter
            if path.startswith("/"):
                # Remove leading slash and convert to Windows path
                path = path.lstrip("/")
                if ":" not in path:  # If no drive letter, assume C:
                    path = "C:\\" + path
            return path.replace("/", "\\")
        else:
            # Convert backslashes to forward slashes for Unix-like systems
            return path.replace("\\", "/")

    @staticmethod
    def normalize_config_paths(config):
        """Normalize all paths in config to system-specific format."""
        if 'repo_root' in config:
            base_path = get_base_path()
            if platform.system() == "Windows":
                # Replace Unix-style base paths with Windows GitHub path
                for unix_path in ["/home/caleb/repo", "/home/caleb/Documents/GitHub/", "/Users/caleb/Documents/GitHub"]:
                    if config['repo_root'].startswith(unix_path):
                        relative_path = config['repo_root'][len(unix_path):].lstrip("/")
                        config['repo_root'] = os.path.join(base_path, relative_path)
                        break
            
            config['repo_root'] = PathConverter.to_system_path(config['repo_root'])
        
        # Convert paths in lists
        for key in ['dirs_to_traverse', 'subdirs_to_exclude', 'files_to_exclude']:
            if key in config and isinstance(config[key], list):
                config[key] = [PathConverter.to_system_path(p) for p in config[key]]
        
        return config

class RepoExporter:
    def __init__(self, config):
        # Convert all paths in config to system-specific format
        config = PathConverter.normalize_config_paths(config)
        self.repo_root = config['repo_root']
        self.export_name = config['export_name']
        self.delimiter = config['delimiter']
        self.dirs_to_traverse = config['dirs_to_traverse']
        self.include_top_level_files = config['include_top_level_files']
        self.included_extensions = config['included_extensions']
        self.subdirs_to_exclude = config.get('subdirs_to_exclude', [])
        self.files_to_exclude = config.get('files_to_exclude', [])
        self.depth = config.get('depth', -1)  # Default to -1 for full traversal
        self.dump_config = config.get('dump_config', False)
        self.exhaustive_dir_tree = config.get('exhaustive_dir_tree', False)
        self.blacklisted_dirs = ['__pycache__']  # Blacklist of subdirs to always omit
        self.blacklisted_dirs.extend(['.git', '.venv', '.vscode'])  # Add common hidden dirs
        self.blacklisted_files = ['uv.lock', 'LICENSE']  # Blacklist of files to always omit
        self.files_to_include = config.get('files_to_include', [])  # Additional files to include explicitly
        self.output_file = self.get_output_file_path()
        self.files_to_exclude.append(os.path.basename(self.output_file))  # Add output file to exclude list
        self.always_exclude_patterns = config.get('always_exclude_patterns', ['export.txt'])
        self.exported_files_count = {}
        self.dirs_for_tree = config.get('dirs_for_tree', [])
        self.total_lines = 0
        self.line_counts_by_file = {}  # Store line counts by relative file path
        self.line_counts_by_dir = {}   # Store aggregated line counts by directory

    def convert_ipynb_to_md(self, notebook_content):
        notebook = nbformat.reads(notebook_content, as_version=4)
        
        # Clear outputs
        clear_output = ClearOutputPreprocessor()
        clear_output.preprocess(notebook, {})
        
        markdown_exporter = MarkdownExporter()
        markdown_content, _ = markdown_exporter.from_notebook_node(notebook)
        return markdown_content
    
    def get_output_file_path(self):
        if os.path.isabs(self.export_name):
            return PathConverter.to_system_path(self.export_name)
        else:
            return PathConverter.to_system_path(os.path.join(self.repo_root, self.export_name))

    def write_to_file(self, content, file_path=None, mode='a'):
        with open(self.output_file, mode, encoding='utf-8') as f:
            if file_path:
                extension = os.path.splitext(file_path)[1]
                self.exported_files_count[extension] = self.exported_files_count.get(extension, 0) + 1
                lines = content.count('\n') + 1
                self.total_lines += lines
                
                # Store line count for this file
                self.line_counts_by_file[file_path] = lines
                
                # Add a note for converted ipynb files
                if extension == '.ipynb':
                    f.write(f"{self.delimiter}\nFull Path: {file_path}\n(NOTE: ipynb notebook converted to md)\n\n{content}\n\n")
                else:
                    f.write(f"{self.delimiter}\nFull Path: {file_path}\n\n{content}\n\n")
            else:
                f.write(f"{content}\n")

    def should_exclude_file(self, file_path):
        relative_path = os.path.relpath(file_path, self.repo_root)
        filename = os.path.basename(file_path)
        
        # First check blacklisted files
        if filename in self.blacklisted_files:
            return True
        
        # Then check always_exclude_patterns
        if any(filename.endswith(pattern) for pattern in self.always_exclude_patterns):
            return True
        
        # Finally check configured exclusions
        return any(relative_path.endswith(exclude) for exclude in self.files_to_exclude)

    def should_exclude_dir(self, dir_path):
        dir_name = os.path.basename(dir_path)
        
        # First check blacklisted dirs
        if dir_name in self.blacklisted_dirs:
            return True
        
        # Then check configured exclusions
        relative_path = os.path.relpath(dir_path, self.repo_root)
        relative_path = PathConverter.to_system_path(relative_path)
        return any(relative_path.startswith(PathConverter.to_system_path(exclude.rstrip('*'))) 
                  for exclude in self.subdirs_to_exclude)

    def traverse_directory(self, directory):
        # Ensure the directory path is absolute
        abs_directory = os.path.join(self.repo_root, directory)

        if not os.path.exists(abs_directory):
            print(f"Warning: Directory {abs_directory} does not exist. Skipping.")
            return

        for root, dirs, files in os.walk(abs_directory):
            dirs[:] = [d for d in dirs if not self.should_exclude_dir(os.path.join(root, d))]

            for file in files:
                file_path = os.path.join(root, file)
                if self.should_exclude_file(file_path):
                    continue
                file_extension = os.path.splitext(file)[1]
                if self.included_extensions == 'all' or file_extension in self.included_extensions:
                    relative_path = os.path.relpath(file_path, self.repo_root)
                    try:
                        with open(file_path, 'r', encoding='utf-8') as f:
                            content = f.read()
                        
                        # Convert ipynb to markdown if necessary
                        if file_extension == '.ipynb':
                            content = self.convert_ipynb_to_md(content)
                        
                        self.write_to_file(content, relative_path)
                    except Exception as e:
                        print(f"Error reading file {file_path}: {str(e)}")

    def include_specific_files(self, root_dir):
        """Include specific files, supporting both relative and absolute paths."""
        for file_path in self.files_to_include:
            if os.path.isabs(file_path):
                # Handle absolute paths directly
                if os.path.exists(file_path) and not self.should_exclude_file(file_path):
                    try:
                        with open(file_path, 'r', encoding='utf-8') as f:
                            content = f.read()
                        relative_path = os.path.relpath(file_path, self.repo_root)
                        self.write_to_file(content, relative_path)
                    except Exception as e:
                        print(f"Error reading file {file_path}: {str(e)}")
            else:
                # Original behavior for relative paths
                for root, _, files in os.walk(root_dir):
                    for file in files:
                        curr_path = os.path.join(root, file)
                        relative_path = os.path.relpath(curr_path, root_dir)
                        if relative_path == file_path and not self.should_exclude_file(curr_path):
                            try:
                                with open(curr_path, 'r', encoding='utf-8') as f:
                                    content = f.read()
                                self.write_to_file(content, relative_path)
                            except Exception as e:
                                print(f"Error reading file {curr_path}: {str(e)}")

    def should_include_in_tree(self, dir_path):
        """Determine if a directory should be included in the tree output."""
        dir_name = os.path.basename(dir_path)

        # Check blacklisted dirs first - make this an immediate return
        if dir_name in self.blacklisted_dirs:
            return False

        # Never include hidden directories (starting with .)
        if dir_name.startswith('.'):
            return False

        # If exhaustive_dir_tree is False, filter directories as they would be during traversal
        if not self.exhaustive_dir_tree:
            # Exclude directories that match subdirs_to_exclude
            if self.should_exclude_dir(dir_path):
                return False
            
            # Also ensure this directory is within one of our allowed traversal directories
            abs_dir_path = os.path.abspath(dir_path)
            allowed = False
            for d in self.dirs_to_traverse:
                allowed_dir = os.path.abspath(os.path.join(self.repo_root, d))
                if os.path.commonprefix([abs_dir_path, allowed_dir]) == allowed_dir:
                    allowed = True
                    break
            if not allowed:
                return False

        # If specific dirs are specified, only include those
        if self.dirs_for_tree:
            relative_path = os.path.relpath(dir_path, self.repo_root)
            return any(relative_path == d or relative_path.startswith(d + os.sep) 
                    for d in self.dirs_for_tree)

        return True

    def compute_directory_line_counts(self):
        """
        Compute the total line counts for each directory.
        We'll sum the line counts of all files under each directory.
        """
        self.line_counts_by_dir = {}

        # Initialize directories found from the exported files
        for rel_file_path, lines in self.line_counts_by_file.items():
            # Add line counts up the chain of directories
            parts = rel_file_path.split(os.sep)
            for i in range(1, len(parts)):
                dir_path = os.sep.join(parts[:i])  # partial path representing directory
                self.line_counts_by_dir[dir_path] = self.line_counts_by_dir.get(dir_path, 0) + lines

    def get_line_count_for_path(self, rel_path):
        """
        Return the line count for a given file or directory relative path.
        If it's a file, look up in line_counts_by_file.
        If it's a directory, look up in line_counts_by_dir.
        If not found, return 0.
        """
        if os.path.isfile(os.path.join(self.repo_root, rel_path)):
            return self.line_counts_by_file.get(rel_path, 0)
        else:
            return self.line_counts_by_dir.get(rel_path, 0)

    def get_directory_tree(self, directory, prefix='', current_depth=0, lines_word_used=False):
        """Generate a string representation of the directory tree with line counts."""
        if self.depth != -1 and current_depth > self.depth:
            return f"{prefix}│   └── (omitted)\n", lines_word_used

        tree_str = ''
        items = sorted(os.listdir(directory))
        
        # Filter items based on visibility rules
        visible_items = []
        for item in items:
            path = os.path.join(directory, item)
            if os.path.isfile(path):
                # Check if file was exported
                rel_file_path = os.path.relpath(path, self.repo_root)
                if rel_file_path in self.line_counts_by_file:  # only include if exported
                    visible_items.append(item)
            elif os.path.isdir(path):
                if self.should_include_in_tree(path):
                    visible_items.append(item)
        
        for i, item in enumerate(visible_items):
            path = os.path.join(directory, item)
            connector = '├── ' if i < len(visible_items) - 1 else '└── '
            rel_path = os.path.relpath(path, self.repo_root)
            line_count = self.get_line_count_for_path(rel_path)
            
            # Determine how to print line counts
            if not lines_word_used:
                # Print lines_word once
                line_str = f"({line_count} lines)" if line_count > 0 else "(0 lines)"
                lines_word_used = True
            else:
                line_str = f"({line_count})"
            
            tree_str += f"{prefix}{connector}{item} {line_str}\n"
            if os.path.isdir(path):
                extension = '' if i < len(visible_items) - 1 else '    '
                subtree_str, lines_word_used = self.get_directory_tree(path, prefix + extension + '│   ', current_depth + 1, lines_word_used)
                tree_str += subtree_str

        return tree_str, lines_word_used

    def export_repo(self):
        # Clear the output file before starting
        with open(self.output_file, 'w', encoding='utf-8') as f:
            pass

        # Write the export configuration to the output file if dump_config is True
        if self.dump_config:
            self.write_to_file(f"Export Configuration:\n{json.dumps(vars(self), indent=2)}", mode='w')

        # Export top-level files (if requested)
        if self.include_top_level_files == 'all':
            for item in os.listdir(self.repo_root):
                item_path = os.path.join(self.repo_root, item)
                if os.path.isfile(item_path):
                    # Add exclusion check before processing the file
                    if self.should_exclude_file(item_path):
                        continue
                    item_extension = os.path.splitext(item)[1]
                    if self.included_extensions == 'all' or item_extension in self.included_extensions:
                        with open(item_path, 'r', encoding='utf-8') as f:
                            content = f.read()
                        self.write_to_file(content, os.path.relpath(item_path, self.repo_root))
        elif isinstance(self.include_top_level_files, list):
            for file_name in self.include_top_level_files:
                if file_name in self.files_to_exclude:
                    continue
                file_path = os.path.join(self.repo_root, file_name)
                if os.path.exists(file_path):
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                    self.write_to_file(content, file_name)

        # Start traversal from each specified directory in dirs_to_traverse
        for dir in self.dirs_to_traverse:
            self.traverse_directory(dir)

        # Include specific files
        self.include_specific_files(self.repo_root)

        # Compute directory line counts
        self.compute_directory_line_counts()

        # Finally, write the directory tree with line counts
        directory_tree_str, _ = self.get_directory_tree(self.repo_root, current_depth=0, lines_word_used=False)
        header = f"Directory tree, stemming from root \"{self.repo_root}\":\n"
        # Overwrite the file with the directory tree at the top, then append exported files below
        # Actually, the user might prefer directory tree at the top. If so, we can prepend it:
        # Let's prepend it:
        with open(self.output_file, 'r', encoding='utf-8') as original:
            original_content = original.read()
        with open(self.output_file, 'w', encoding='utf-8') as modified:
            modified.write(header)
            modified.write(directory_tree_str)
            if original_content.strip():
                modified.write(self.delimiter + "\n" + original_content)

        # At the end of the script, after all processing
        print(f"Exported to: {self.output_file}")
        print(f"Total number of lines: {self.total_lines}")
        print("Number of exported files by extension:")
        for ext, count in self.exported_files_count.items():
            print(f"{ext}: {count}")

def get_base_path():
    """
    Determine the base path based on the host platform or command-line argument.
    """
    if '--pop' in sys.argv:
        return '/home/caleb/Documents/GitHub/'  # pop-xps popOS system
    elif platform.system() == "Darwin":  # macOS
        return "/Users/caleb/Documents/GitHub"
    elif platform.system() == "Windows":  # Windows
        return r"C:\Users\front\Documents\GitHub"
    else:  # Linux or other (dev server, or polliserve instances)
        return "/home/caleb/repo"

def load_config(config_filename):
    """
    Load configuration from a JSON file and adjust paths if necessary.
    """
    base_path = get_base_path()
    config_path = os.path.join(base_path, "utils/export_repo/configs", config_filename)
    config_path = PathConverter.to_system_path(config_path)
    
    with open(config_path, 'r', encoding='utf-8') as config_file:
        config = json.load(config_file)
    
    # Normalize all paths in config to system-specific format
    config = PathConverter.normalize_config_paths(config)
    return config

def get_default_config(repo_root):
    """
    Return a default configuration when no config file is provided.
    """
    return {
        'repo_root': repo_root,
        'export_name': f"{os.path.basename(repo_root)}_export.txt",
        'delimiter': '---',
        'dirs_to_traverse': ['.'],
        'include_top_level_files': 'all',
        'included_extensions': 'all',
        'subdirs_to_exclude': ['__pycache__'],
        'files_to_exclude': [],
        'depth': 10,
        'exhaustive_dir_tree': False,
        'files_to_include': [],
        'always_exclude_patterns': ['export.txt'],
        'dump_config': False
    }

def main():
    args = sys.argv[1:]
    config_filename = None
    pop_flag = False
    dump_config = False

    for arg in args:
        if arg == '--pop':
            pop_flag = True
        elif arg == '--dump-config':
            dump_config = True
        elif not arg.startswith('--'):
            config_filename = arg

    if not config_filename:
        print("Usage: python script.py [--pop] [--dump-config] <config_filename> or <repo_root>")
        sys.exit(1)

    if os.path.isdir(config_filename):
        # If the argument is a directory, use it as repo_root with default config
        config = get_default_config(config_filename)
    else:
        # If not a directory, treat it as a config file name
        if not config_filename.endswith('.json'):
            config_filename += '.json'
        config = load_config(config_filename)

    # Adjust repo_root based on pop_flag
    if pop_flag:
        base_path = '/home/caleb/Documents/GitHub/'
        config['repo_root'] = config['repo_root'].replace("/home/caleb/repo", base_path)

    # Set dump_config based on command-line flag
    config['dump_config'] = dump_config

    exporter = RepoExporter(config)
    exporter.export_repo()

if __name__ == "__main__":
    main()
</export_repo_to_txt.py>
<readme.md>
# Export Repository to Text File

## Overview

This tool exports repository content to a text file, including directory structures and file contents. It provides flexible configuration options to control what is included in the export.

## Usage

```bash
python export_repo_to_txt.py <config_file>.json [--dump-config]
```

or

```bash
python export_repo_to_txt.py <repo_root> [--dump-config]
```

## Configuration Parameters

- `repo_root`: Path to the repository's root directory.
- `export_name`: Name or path for the export file. If a path, exports there; if a name, exports to `repo_root`.
- `delimiter`: Separator string for entries in the export file.
- `dirs_to_traverse`: List of directories within `repo_root` for full traversal and export.
- `dirs_for_tree`: List of specific directories to include in the directory tree output. If empty, includes all non-hidden directories.
- `files_to_include`: List of specific files to include in the export, regardless of their location in the repository.
- `include_top_level_files`: Specifies top-level files for inclusion. Set to `"all"` for all files, or list specific files.
- `included_extensions`: File extensions to include. Use `"all"` for all extensions.
- `subdirs_to_exclude`: List of subdirectory names or paths to exclude from traversal.
- `files_to_exclude`: List of file names to exclude from the export.
- `always_exclude_patterns`: List of filename patterns to always exclude (e.g., ["export.txt"]).
- `depth`: Depth of directory traversal. `-1` for full traversal (default).
- `exhaustive_dir_tree`: If `true`, exports full directory tree regardless of other settings.
- `dump_config`: If `true`, dumps the export configuration JSON at the top of the output file.

## Example Configuration

```json
{
  "repo_root": "/home/user/myrepo",
  "export_name": "repo_export.txt",
  "delimiter": "----",
  "dirs_to_traverse": ["src", "docs"],
  "files_to_include": ["README.md", "config.json"],
  "include_top_level_files": "all",
  "included_extensions": [".py", ".md", ".json"],
  "subdirs_to_exclude": ["tests", "build"],
  "files_to_exclude": ["secrets.yaml"],
  "always_exclude_patterns": ["export.txt", "*.log"],
  "depth": -1,
  "exhaustive_dir_tree": false,
  "dump_config": false
}
```

## Notes

- The tool will automatically exclude the output file from the export.
- If no config file is provided, default settings will be used.
- The `subdirs_to_exclude` option supports partial paths (e.g., "foo/bar" will exclude all "bar" directories under any "foo" directory).
- Use `always_exclude_patterns` for files you want to exclude regardless of their location or other inclusion rules.
- To include the export configuration in the output file, use the `--dump-config` flag when running the script.
- Hidden directories (starting with '.') are automatically excluded from the directory tree.
- Use `dirs_for_tree` to explicitly specify which directories should appear in the directory tree output. If not specified, all non-hidden directories will be included.


## Example Configurations

### NextJS Project

```json
{
  "repo_root": "/home/user/nextjs-project",
  "export_name": "nextjs_project_export.txt",
  "delimiter": "----",
  "dirs_to_traverse": ["components", "pages", "styles", "public"],
  "dirs_for_tree": ["components", "pages", "styles"],  // Only show main app directories
  "include_top_level_files": ["package.json", "next.config.js"],
  "included_extensions": [".js", ".jsx", ".ts", ".tsx", ".css"]
}
```

### Python Project

```json
{
  "repo_root": "/home/user/python-project",
  "export_name": "python_project_export.txt",
  "delimiter": "----",
  "dirs_to_traverse": ["src", "tests", "docs"],
  "dirs_for_tree": ["src", "docs"],  // Only show source and documentation
  "files_to_include": ["requirements.txt", "setup.py"],
  "include_top_level_files": "all",
  "included_extensions": [".py", ".md", ".yml"],
  "subdirs_to_exclude": ["__pycache__", ".venv"]
}
```

## Setting up the `export_repo` alias

### On macOS

1. Open your terminal.

2. Open your shell configuration file (for zsh, it's usually `~/.zshrc`):
   ```bash
   nano ~/.zshrc
   ```

3. Add the following line at the end of the file:
   ```bash
   alias export_repo='/Users/caleb/Documents/GitHub/utils/.venv/bin/python /Users/caleb/Documents/GitHub/utils/export_repo/export_repo_to_txt.py'
   ```

4. Save the file and exit the editor (in nano, press Ctrl+X, then Y, then Enter).

5. Reload your shell configuration:
   ```bash
   source ~/.zshrc
   ```

### On PopOS (Linux)

1. Open your terminal.

2. Open your shell configuration file (for bash, it's usually `~/.bashrc`):
   ```bash
   nano ~/.bashrc
   ```

3. Add the following line at the end of the file:
   ```bash
   alias export_repo='/home/caleb/Documents/GitHub/utils/.venv/bin/python /home/caleb/Documents/GitHub/utils/export_repo/export_repo_to_txt.py --pop'
   ```

4. Save the file and exit the editor (in nano, press Ctrl+X, then Y, then Enter).

5. Reload your shell configuration:
   ```bash
   source ~/.bashrc
   ```

### On Windows (PowerShell)

1. Open PowerShell.

2. First, check if you already have a PowerShell profile:
   ```powershell
   Test-Path $PROFILE
   ```

3. If it returns False, create one:
   ```powershell
   New-Item -Path $PROFILE -Type File -Force
   ```

4. Open your PowerShell profile in a text editor:
   ```powershell
   notepad $PROFILE
   ```

5. Add this function to your profile:
   ```powershell
   function export_repo {
       & "C:\Users\front\Documents\GitHub\utils\.venv\Scripts\python.exe" "C:\Users\front\Documents\GitHub\utils\export_repo\export_repo_to_txt.py" $args
   }
   ```

6. Save the file and reload your profile:
   ```powershell
   . $PROFILE
   ```

Now you can use the `export_repo` command from anywhere in your terminal on macOS, PopOS, or Windows PowerShell. For example:

```bash
export_repo hFormer-codeOnly
# or 
export_repo /path/to/your/repo --dump-config
```

Notes: 
- On PopOS, the `--pop` flag is automatically included in the alias to ensure the correct base path is used
- On Windows, make sure you're using PowerShell and not Command Prompt (cmd.exe) as this alias will only work in PowerShell
</readme.md>
<dirtree>
Directory tree, stemming from root "/home/caleb/repo/utils/export_repo":
├── configs (455 lines)
│   ├── autocrop.json (10)
│   ├── bulk_dl.json (14)
│   ├── cosm-c360-tools.json (12)
│   ├── export_repo.json (10)
│   ├── ezmd.json (11)
│   ├── h5merge-mini.json (12)
│   ├── h5merge.json (12)
│   ├── h5pull.json (11)
│   ├── hFormer0-serve.json (10)
│   ├── ibrida.json (12)
│   ├── ibridaDB_v0r1_export.json (12)
│   ├── ibridaDB_v0rX.json (12)
│   ├── ibridaV2_generator.json (15)
│   ├── ladybird_data.json (13)
│   ├── metaformer.json (11)
│   ├── metaformer1.json (12)
│   ├── metaformer2.json (11)
│   ├── model-explorer.json (13)
│   ├── nextjs.json (8)
│   ├── polliFormer-COPAP.json (12)
│   ├── polliFormer-Dyn.json (12)
│   ├── polliFormer-blade-angio-0.json (14)
│   ├── polliFormer-buildData.json (15)
│   ├── polliFormer-codeOnly.json (11)
│   ├── polliFormer-configModel.json (14)
│   ├── polliFormer-h5Data.json (13)
│   ├── polliFormer-models-codeOnly.json (11)
│   ├── polliFormer-models.json (11)
│   ├── polliFormer-modelsDyn.json (11)
│   ├── polliFormer-modelsPruned-codeOnly.json (11)
│   ├── polliFormer-modelsPrunedInv-codeOnly.json (11)
│   ├── polliFormer-serve.json (10)
│   ├── polliFormer-tests.json (12)
│   ├── polliFormer.json (13)
│   ├── polliOS-codeOnly.json (13)
│   ├── polliOS.json (13)
│   ├── sam2.json (12)
│   └── sam2_demo.json (15)
└── export_repo_to_txt.py (453)
</dirtree
</codebase>

<bug>
When exporting the ezmd repo with the following config:
```
{
    "repo_root": "/home/caleb/repo/ezmd",
    "export_name": "ezmd_export.txt",
    "delimiter": "----",
    "dirs_to_traverse": ["src", "tests"],
    "subdirs_to_exclude": ["dev"],
    "include_top_level_files": "all",
    "included_extensions": [".py", ".env", ".toml", ".md"],
    "always_exclude_patterns": ["uv.lock", "export.txt", ".log", ".venv", ".gitignore"],
    "exhaustive_dir_tree": false
  }
```
The file '/home/caleb/repo/ezmd/dev/prompts.txt' is incorrectly included in the export. That file should not be included in the export, and should have been caught by several different rules.
</bug>
<discussion>
So, given this dirtree in the target repo:

```
Directory tree, stemming from root "ezmd":
├── .gitignore (172 lines)
├── .python-version (2)
├── README.md (1)
├── dev (73)
│   └── prompts.txt (73)
├── pyproject.toml (12)
├── src (0)
└── tests (0)
```
And the disinclusion of the 'dev' subdir from the dirs_to_traverse list, the inclusion of 'dev' in the subdirs_to_exclude list, and the disinclusion of '.txt' in the "included_extensions" list, there are zero reasons why the prompts.txt file should be included in the export, and several reasons why prompts.txt should have been excluded.
</discussion>
<context>
This is a hodgepodge personal tool that I use to gather context to prompt LLMs. Because I have added additional rules as-needed to get a particular outcome without considering the soundness of the overall ruleset or logical flows, the system is a patchwork, and I'm not surprised that we're encountering this conflict. To reduce the number of headaches that this tool causes me in the future, we should probably design the ruleset de-novo to simplify the logical chains and make the tool easier to use. The reformulated tool should achieve full coverage of the existing feature set, but should have fewer configurable parameters, each with a well-defined scope. My typical usage pattern for this tool is to define a general set of inclusion rules, and then to prune with finer-grained exclusion rules (like exclude_subdir, exclude_file), and to sometimes pull in files outside the scope of the root dir by providing explicit paths.

I am mostly satisfied with the formatting of the export txtfiles generated by this tool (including the dirtree and the file-by-file dumps), but we should make the outputs a bit more LLM-friendly by structuring the outputs with XML tags (instead of the delimiters that we currently use, which are tailored for human eyes). We should wrap the entire output file with a <codebase></codebase> tag. Then, use XML delimeters to separate into <directory_tree> and the <files> sections. Finally, separate each file in the file-by-file dumps (i.e. the codebase_files section) with XML tags-- use the filename as the tag. Let's also use XML tags to group the file-by-file dumps by their directory hierarchy, ex. in this codebase <export_repo.py> would be nested under <export_repo>, which would itself be nested under <files>. Remember that both top-level sections, <files> and <directory_tree> would be nested under the <codebase> tag-- <codebase> wraps the entire export file so that we can paste the output directly into our LLM prompt (I use highly-structured prompts, where each component of the prompt is XML-structured-- you can see an example of this in this very message.)

Note that we may add an interactive mode in the future; the usage pattern here would be to have a general config per-repo but to allow the user to quickly compose tailored exports for different tasks within a TUI.. in most cases, this would be quicker and more maintainable than writing a new config file each time, which can quickly grow to an overwhelming scale. If we implement this feature, we'd use a sidecar file to cache the 10-20 most recently used 'tailored' prompts (globally, and for each of the base per-repo configs) for quick reuse. We would also add a tool version tracking system to the configs/sidecar files to help us deal with breaking changes in the core logic and rulesets. So design the new ruleset and architectures with this future requirement in mind-- not sure if we'll do this today, but it's been on my todo list for a while. 

I've had a few folks on twitter ask to use my tool, but it's grown into such a mess over time that I haven't published it. If I'm satisfied with what we come up with today, I may finally publish this tool-- and I'll throw you some credit! So let's take all the time we need to think deeply and come up with something great.
</context>
<task_0>
Fully trace the ruleset application flow for this ezmd export config. Identify the root cause of the logical flaw that led to the incorrect inclusion of ezmd/dev/prompts.txt in the export.
</task_0>
<task_1>
Exhaustively map the ruleset hierarchy. Identify conflicting rules. Identify rules with overlapping scopes (there are many).
</task_1>
<task_2>
Propose a 'smarter' ruleset that we can use in a reimplementation of this tool. The user should be able to achieve the same level of granularity/control over the export, but the median export config json should be substantially simpler. The user shouldn't need to consult the README every time they need to draft a new export config-- the parameters should have clearly-defined scopes, with descriptive names and a self-evident relational structure.

Think through every aspect of the new system. We'll focus on the core logical structure for now-- let's dial in a production-grade ruleset that optimizes for consistency, simplicity, smart defaults, and granularity.
</task_2>
<task_3>
Review the proposed interactive export_repo tool that I described earlier-- just so we don't forgot the rough guidelines I described in the context section, please summarize and structure the first-draft feature set that I described. While this new feature isn't our focus in this message, I want to keep track of these initial thoughts I had in case we decide to move forward with this feature tonight.

Provide some follow-up questions to clarify the high-level requirements that I'd be looking for in this feature. When we're ready to work on this feature, I'll respond to these and we can iteratively architect a well-considered interactive mode; in the long run I think this might be the most common means by which users interact with our tool. I've been particularly inspired by the 'Repo Prompt' tool that has recently launched (as a dev preview) on TestFlight-- twitter seems to love it, and so I'm inspired to revisit this idea.
</task_3>
<think_slowly_and_deeply>
Take as much time as you need to think about these tasks. This is a pretty fun problem-- identifying the scope and deducing the logical chains in our old tool's rulesets, identifying the key filtering requirements, and then taking the opportunity to conceptualize an optimized ruleset algorithm. Since we're essentially starting from fresh, this is a rare opportunity to design an algorithm from scratch (albeit within the constraints that we need to preserve the ability to configure exports at a granularity at least as fine as the old tool). This is about as close to a pure logics puzzle as it gets-- so let's take our time and think into deep conceptual space, and we can bring things back to our actual software implementation later.
</<think_slowly_and_deeply>
---
</file: new_export_repo_prompts.xml>

<file: readme.md>
# Export Repository to Text File

## Overview

This tool exports repository content to a text file, including directory structures and file contents. It provides flexible configuration options to control what is included in the export.

## Usage

```bash
python export_repo_to_txt.py <config_file>.json [--dump-config]
```

or

```bash
python export_repo_to_txt.py <repo_root> [--dump-config]
```

## Configuration Parameters

- `repo_root`: Path to the repository's root directory.
- `export_name`: Name or path for the export file. If a path, exports there; if a name, exports to `repo_root`.
- `delimiter`: Separator string for entries in the export file.
- `dirs_to_traverse`: List of directories within `repo_root` for full traversal and export.
- `dirs_for_tree`: List of specific directories to include in the directory tree output. If empty, includes all non-hidden directories.
- `files_to_include`: List of specific files to include in the export, regardless of their location in the repository.
- `include_top_level_files`: Specifies top-level files for inclusion. Set to `"all"` for all files, or list specific files.
- `included_extensions`: File extensions to include. Use `"all"` for all extensions.
- `subdirs_to_exclude`: List of subdirectory names or paths to exclude from traversal.
- `files_to_exclude`: List of file names to exclude from the export.
- `always_exclude_patterns`: List of filename patterns to always exclude (e.g., ["export.txt"]).
- `depth`: Depth of directory traversal. `-1` for full traversal (default).
- `exhaustive_dir_tree`: If `true`, exports full directory tree regardless of other settings.
- `dump_config`: If `true`, dumps the export configuration JSON at the top of the output file.

## Example Configuration

```json
{
  "repo_root": "/home/user/myrepo",
  "export_name": "repo_export.txt",
  "delimiter": "----",
  "dirs_to_traverse": ["src", "docs"],
  "files_to_include": ["README.md", "config.json"],
  "include_top_level_files": "all",
  "included_extensions": [".py", ".md", ".json"],
  "subdirs_to_exclude": ["tests", "build"],
  "files_to_exclude": ["secrets.yaml"],
  "always_exclude_patterns": ["export.txt", "*.log"],
  "depth": -1,
  "exhaustive_dir_tree": false,
  "dump_config": false
}
```

## Notes

- The tool will automatically exclude the output file from the export.
- If no config file is provided, default settings will be used.
- The `subdirs_to_exclude` option supports partial paths (e.g., "foo/bar" will exclude all "bar" directories under any "foo" directory).
- Use `always_exclude_patterns` for files you want to exclude regardless of their location or other inclusion rules.
- To include the export configuration in the output file, use the `--dump-config` flag when running the script.
- Hidden directories (starting with '.') are automatically excluded from the directory tree.
- Use `dirs_for_tree` to explicitly specify which directories should appear in the directory tree output. If not specified, all non-hidden directories will be included.


## Example Configurations

### NextJS Project

```json
{
  "repo_root": "/home/user/nextjs-project",
  "export_name": "nextjs_project_export.txt",
  "delimiter": "----",
  "dirs_to_traverse": ["components", "pages", "styles", "public"],
  "dirs_for_tree": ["components", "pages", "styles"],  // Only show main app directories
  "include_top_level_files": ["package.json", "next.config.js"],
  "included_extensions": [".js", ".jsx", ".ts", ".tsx", ".css"]
}
```

### Python Project

```json
{
  "repo_root": "/home/user/python-project",
  "export_name": "python_project_export.txt",
  "delimiter": "----",
  "dirs_to_traverse": ["src", "tests", "docs"],
  "dirs_for_tree": ["src", "docs"],  // Only show source and documentation
  "files_to_include": ["requirements.txt", "setup.py"],
  "include_top_level_files": "all",
  "included_extensions": [".py", ".md", ".yml"],
  "subdirs_to_exclude": ["__pycache__", ".venv"]
}
```

## Setting up the `export_repo` alias

### On macOS

1. Open your terminal.

2. Open your shell configuration file (for zsh, it's usually `~/.zshrc`):
   ```bash
   nano ~/.zshrc
   ```

3. Add the following line at the end of the file:
   ```bash
   alias export_repo='/Users/caleb/Documents/GitHub/utils/.venv/bin/python /Users/caleb/Documents/GitHub/utils/export_repo/export_repo_to_txt.py'
   ```

4. Save the file and exit the editor (in nano, press Ctrl+X, then Y, then Enter).

5. Reload your shell configuration:
   ```bash
   source ~/.zshrc
   ```

### On PopOS (Linux)

1. Open your terminal.

2. Open your shell configuration file (for bash, it's usually `~/.bashrc`):
   ```bash
   nano ~/.bashrc
   ```

3. Add the following line at the end of the file:
   ```bash
   alias export_repo='/home/caleb/Documents/GitHub/utils/.venv/bin/python /home/caleb/Documents/GitHub/utils/export_repo/export_repo_to_txt.py --pop'
   ```

4. Save the file and exit the editor (in nano, press Ctrl+X, then Y, then Enter).

5. Reload your shell configuration:
   ```bash
   source ~/.bashrc
   ```

### On Windows (PowerShell)

1. Open PowerShell.

2. First, check if you already have a PowerShell profile:
   ```powershell
   Test-Path $PROFILE
   ```

3. If it returns False, create one:
   ```powershell
   New-Item -Path $PROFILE -Type File -Force
   ```

4. Open your PowerShell profile in a text editor:
   ```powershell
   notepad $PROFILE
   ```

5. Add this function to your profile:
   ```powershell
   function export_repo {
       & "C:\Users\front\Documents\GitHub\utils\.venv\Scripts\python.exe" "C:\Users\front\Documents\GitHub\utils\export_repo\export_repo_to_txt.py" $args
   }
   ```

6. Save the file and reload your profile:
   ```powershell
   . $PROFILE
   ```

Now you can use the `export_repo` command from anywhere in your terminal on macOS, PopOS, or Windows PowerShell. For example:

```bash
export_repo hFormer-codeOnly
# or 
export_repo /path/to/your/repo --dump-config
```

Notes: 
- On PopOS, the `--pop` flag is automatically included in the alias to ensure the correct base path is used
- On Windows, make sure you're using PowerShell and not Command Prompt (cmd.exe) as this alias will only work in PowerShell
</file: readme.md>

<file: export_repo_to_txt.py>
import platform
import os
import json
import sys
import nbformat
from nbconvert import MarkdownExporter
from nbconvert.preprocessors import ClearOutputPreprocessor

class PathConverter:
    @staticmethod
    def to_system_path(path: str) -> str:
        """
        Convert the given path to the current system's native format.
        On Windows, forward slashes become backslashes, etc.
        """
        if platform.system() == "Windows":
            # Convert forward slashes to backslashes and handle drive letter
            if path.startswith("/"):
                # Remove leading slash and convert to Windows path
                path = path.lstrip("/")
                if ":" not in path:  # If no drive letter, assume C:
                    path = "C:\\" + path
            return path.replace("/", "\\")
        else:
            # Convert backslashes to forward slashes for Unix-like systems
            return path.replace("\\", "/")

    @staticmethod
    def normalize_config_paths(config: dict) -> dict:
        """
        Normalize all relevant paths in the config to the current system's format.
        """
        if 'repo_root' in config:
            base_path = get_base_path()
            if platform.system() == "Windows":
                # Replace Unix-style base paths with Windows GitHub path
                for unix_path in ["/home/caleb/repo", "/home/caleb/Documents/GitHub/", "/Users/caleb/Documents/GitHub"]:
                    if config['repo_root'].startswith(unix_path):
                        relative_path = config['repo_root'][len(unix_path):].lstrip("/")
                        config['repo_root'] = os.path.join(base_path, relative_path)
                        break
            
            config['repo_root'] = PathConverter.to_system_path(config['repo_root'])
        
        # Convert path lists
        for key in ['dirs_to_traverse', 'subdirs_to_exclude', 'files_to_exclude']:
            if key in config and isinstance(config[key], list):
                config[key] = [PathConverter.to_system_path(p) for p in config[key]]
        
        return config

class RepoExporter:
    def __init__(self, config: dict, config_filename: str = None):
        """
        Initialize the RepoExporter with the given config dictionary.
        :param config: The loaded or constructed configuration object.
        :param config_filename: Optional name of the config file (if used), 
                               for placing in the output XML tags.
        """
        # Convert all paths in config to system-specific format
        config = PathConverter.normalize_config_paths(config)
        self.repo_root = config['repo_root']
        self.export_name = config['export_name']
        self.dirs_to_traverse = config['dirs_to_traverse']
        self.include_top_level_files = config['include_top_level_files']
        self.included_extensions = config['included_extensions']
        self.subdirs_to_exclude = config.get('subdirs_to_exclude', [])
        self.files_to_exclude = config.get('files_to_exclude', [])
        self.depth = config.get('depth', -1)  # Default to -1 for full traversal
        self.dump_config = config.get('dump_config', False)
        self.exhaustive_dir_tree = config.get('exhaustive_dir_tree', False)

        # Additional rules and blacklists
        self.blacklisted_dirs = ['__pycache__', '.git', '.venv', '.vscode']
        self.blacklisted_files = ['uv.lock', 'LICENSE']
        self.files_to_include = config.get('files_to_include', [])
        self.always_exclude_patterns = config.get('always_exclude_patterns', ['export.txt'])
        self.dirs_for_tree = config.get('dirs_for_tree', [])

        # Bookkeeping
        self.config_filename = config_filename
        self.exported_files_count = {}
        self.total_lines = 0
        self.line_counts_by_file = {}
        self.line_counts_by_dir = {}
        
        # Make sure the final output path is correct
        self.output_file = self.get_output_file_path()
        # Avoid re-exporting the output file
        self.files_to_exclude.append(os.path.basename(self.output_file))

        # Memory buffer for file contents (rather than writing them incrementally)
        # Each entry is a tuple: (rel_path, content, is_ipynb_converted)
        self.file_contents = []

    def get_output_file_path(self) -> str:
        """
        Return the absolute path for the export file, 
        converting paths if necessary.
        """
        if os.path.isabs(self.export_name):
            return PathConverter.to_system_path(self.export_name)
        else:
            return PathConverter.to_system_path(os.path.join(self.repo_root, self.export_name))

    def convert_ipynb_to_md(self, notebook_content: str) -> str:
        """
        Convert an IPython notebook JSON string to Markdown by clearing outputs
        and using nbconvert's MarkdownExporter.
        """
        notebook = nbformat.reads(notebook_content, as_version=4)
        
        # Clear outputs
        clear_output = ClearOutputPreprocessor()
        clear_output.preprocess(notebook, {})

        # Convert to markdown
        markdown_exporter = MarkdownExporter()
        markdown_content, _ = markdown_exporter.from_notebook_node(notebook)
        return markdown_content

    def store_file_content(self, content: str, file_path: str, ipynb_converted: bool = False):
        """
        Cache the file's content in memory, track line counts, 
        and update the total lines + extension stats.
        """
        extension = os.path.splitext(file_path)[1]
        self.exported_files_count[extension] = self.exported_files_count.get(extension, 0) + 1
        
        line_count = content.count('\n') + 1
        self.total_lines += line_count
        self.line_counts_by_file[file_path] = line_count

        # Store the file content in memory for final output
        self.file_contents.append((file_path, content, ipynb_converted))

    def should_exclude_file(self, file_path: str) -> bool:
        """
        Return True if the file_path should be skipped 
        based on blacklists, always_exclude_patterns, and user config.
        """
        relative_path = os.path.relpath(file_path, self.repo_root)
        filename = os.path.basename(file_path)
        
        # Check blacklisted files
        if filename in self.blacklisted_files:
            return True
        
        # Patterns
        if any(filename.endswith(pattern) for pattern in self.always_exclude_patterns):
            return True
        
        # Config-based exclude
        return any(relative_path.endswith(exclude) for exclude in self.files_to_exclude)

    def should_exclude_dir(self, dir_path: str) -> bool:
        """
        Return True if the directory should be skipped 
        based on blacklists and user config.
        """
        dir_name = os.path.basename(dir_path)

        # Blacklisted names
        if dir_name in self.blacklisted_dirs:
            return True
        
        # Config-based excludes
        relative_path = os.path.relpath(dir_path, self.repo_root)
        relative_path = PathConverter.to_system_path(relative_path)
        return any(relative_path.startswith(PathConverter.to_system_path(exclude.rstrip('*'))) 
                   for exclude in self.subdirs_to_exclude)

    def traverse_directory(self, directory: str):
        """
        Walk through 'directory' within the repo_root, 
        read all matching files, and store their contents in memory.
        """
        abs_directory = os.path.join(self.repo_root, directory)

        if not os.path.exists(abs_directory):
            print(f"Warning: Directory {abs_directory} does not exist. Skipping.")
            return

        for root, dirs, files in os.walk(abs_directory):
            # Filter out excluded subdirs
            dirs[:] = [d for d in dirs if not self.should_exclude_dir(os.path.join(root, d))]

            # Check each file
            for file in files:
                file_path = os.path.join(root, file)
                if self.should_exclude_file(file_path):
                    continue
                file_extension = os.path.splitext(file)[1]
                # Check if extension is included
                if self.included_extensions == 'all' or file_extension in self.included_extensions:
                    relative_path = os.path.relpath(file_path, self.repo_root)
                    try:
                        with open(file_path, 'r', encoding='utf-8') as f:
                            content = f.read()
                        # Convert .ipynb to .md if necessary
                        if file_extension == '.ipynb':
                            content = self.convert_ipynb_to_md(content)
                            self.store_file_content(content, relative_path, ipynb_converted=True)
                        else:
                            self.store_file_content(content, relative_path, ipynb_converted=False)
                    except Exception as e:
                        print(f"Error reading file {file_path}: {str(e)}")

    def include_specific_files(self):
        """
        Include user-specified files in self.files_to_include, 
        supporting absolute or relative paths.
        """
        for file_path in self.files_to_include:
            # Check absolute vs. relative
            if os.path.isabs(file_path):
                # Handle absolute path directly
                if os.path.exists(file_path) and not self.should_exclude_file(file_path):
                    try:
                        with open(file_path, 'r', encoding='utf-8') as f:
                            content = f.read()
                        relative_path = os.path.relpath(file_path, self.repo_root)
                        extension = os.path.splitext(file_path)[1]
                        if extension == '.ipynb':
                            content = self.convert_ipynb_to_md(content)
                            self.store_file_content(content, relative_path, ipynb_converted=True)
                        else:
                            self.store_file_content(content, relative_path, ipynb_converted=False)
                    except Exception as e:
                        print(f"Error reading file {file_path}: {str(e)}")
            else:
                # It's a relative path from repo_root
                abs_root = os.path.abspath(self.repo_root)
                for root, _, files in os.walk(abs_root):
                    for file in files:
                        curr_path = os.path.join(root, file)
                        rel_path = os.path.relpath(curr_path, abs_root)
                        if rel_path == file_path and not self.should_exclude_file(curr_path):
                            try:
                                with open(curr_path, 'r', encoding='utf-8') as f:
                                    content = f.read()
                                extension = os.path.splitext(curr_path)[1]
                                if extension == '.ipynb':
                                    content = self.convert_ipynb_to_md(content)
                                    self.store_file_content(content, file_path, ipynb_converted=True)
                                else:
                                    self.store_file_content(content, file_path, ipynb_converted=False)
                            except Exception as e:
                                print(f"Error reading file {curr_path}: {str(e)}")

    def should_include_in_tree(self, dir_path: str) -> bool:
        """
        Return True if 'dir_path' should appear in the directory tree.
        This logic respects blacklists, user excludes, and (optionally) self.exhaustive_dir_tree.
        """
        dir_name = os.path.basename(dir_path)

        # Immediately drop blacklisted or hidden
        if dir_name in self.blacklisted_dirs or dir_name.startswith('.'):
            return False

        # If exhaustive_dir_tree is off, filter further
        if not self.exhaustive_dir_tree:
            if self.should_exclude_dir(dir_path):
                return False
            # Also ensure directory is within dirs_to_traverse
            abs_dir_path = os.path.abspath(dir_path)
            allowed = False
            for d in self.dirs_to_traverse:
                allowed_dir = os.path.abspath(os.path.join(self.repo_root, d))
                if os.path.commonprefix([abs_dir_path, allowed_dir]) == allowed_dir:
                    allowed = True
                    break
            if not allowed:
                return False

        # If dirs_for_tree is specified, only include matches
        if self.dirs_for_tree:
            relative_path = os.path.relpath(dir_path, self.repo_root)
            return any(
                relative_path == d or relative_path.startswith(d + os.sep)
                for d in self.dirs_for_tree
            )

        return True

    def compute_directory_line_counts(self):
        """
        After reading all file contents, sum up the line counts
        for each directory that contains them.
        """
        self.line_counts_by_dir = {}
        for rel_file_path, lines in self.line_counts_by_file.items():
            parts = rel_file_path.split(os.sep)
            # Add line counts up the chain of directories
            for i in range(1, len(parts)):
                dir_path = os.sep.join(parts[:i])
                self.line_counts_by_dir[dir_path] = self.line_counts_by_dir.get(dir_path, 0) + lines

    def get_line_count_for_path(self, rel_path: str) -> int:
        """
        Return the line count for the given rel_path 
        (if file) or sum for all files under that directory.
        """
        full_path = os.path.join(self.repo_root, rel_path)
        if os.path.isfile(full_path):
            return self.line_counts_by_file.get(rel_path, 0)
        else:
            return self.line_counts_by_dir.get(rel_path, 0)

    def get_directory_tree(self, directory: str, prefix: str = '', current_depth: int = 0, lines_word_used: bool = False):
        """
        Produce a string representation of 'directory' in an ASCII tree format, 
        labeling each item with either "(X lines)" or "(X)" (once "lines" has appeared).
        """
        if self.depth != -1 and current_depth > self.depth:
            return f"{prefix}   (omitted)\n", lines_word_used

        tree_str = ''
        items = sorted(os.listdir(directory))
        visible_items = []
        for item in items:
            path = os.path.join(directory, item)
            if os.path.isfile(path):
                # Show file if it was actually exported
                rel_file_path = os.path.relpath(path, self.repo_root)
                if rel_file_path in self.line_counts_by_file:
                    visible_items.append(item)
            elif os.path.isdir(path):
                if self.should_include_in_tree(path):
                    visible_items.append(item)

        for i, item in enumerate(visible_items):
            path = os.path.join(directory, item)
            rel_path = os.path.relpath(path, self.repo_root)
            line_count = self.get_line_count_for_path(rel_path)

            # Use ASCII connectors
            connector = '|-- ' if i < len(visible_items) - 1 else '\\-- '

            if not lines_word_used:
                # first time we show line counts, add the word "lines"
                line_str = f"({line_count} lines)" if line_count > 0 else "(0 lines)"
                lines_word_used = True
            else:
                line_str = f"({line_count})"

            tree_str += f"{prefix}{connector}{item} {line_str}\n"
            if os.path.isdir(path):
                # Subtree indentation: align with the connector
                sub_prefix = prefix + ("|   " if i < len(visible_items) - 1 else "    ")
                subtree_str, lines_word_used = self.get_directory_tree(path, sub_prefix, current_depth + 1, lines_word_used)
                tree_str += subtree_str

        return tree_str, lines_word_used

    def export_repo(self):
        """
        Main routine:
        1) Possibly gather top-level files,
        2) Recursively gather all files from dirs_to_traverse,
        3) Include any extra user-specified files,
        4) Compute line counts,
        5) Write a final output file that uses XML-style tags:
           <codebase_context> 
              <repo export config: ...> ... </repo export config: ...>
              <dirtree: ...> ... </dirtree: ...>
              <file: ...> ... </file: ...>
           </codebase_context>
        """
        # 1) Optionally include top-level files
        if self.include_top_level_files == 'all':
            for item in os.listdir(self.repo_root):
                item_path = os.path.join(self.repo_root, item)
                if os.path.isfile(item_path) and not self.should_exclude_file(item_path):
                    file_extension = os.path.splitext(item)[1]
                    if self.included_extensions == 'all' or file_extension in self.included_extensions:
                        try:
                            with open(item_path, 'r', encoding='utf-8') as f:
                                content = f.read()
                            if file_extension == '.ipynb':
                                content = self.convert_ipynb_to_md(content)
                                self.store_file_content(content, os.path.relpath(item_path, self.repo_root), True)
                            else:
                                self.store_file_content(content, os.path.relpath(item_path, self.repo_root), False)
                        except Exception as e:
                            print(f"Error reading top-level file {item_path}: {str(e)}")
        elif isinstance(self.include_top_level_files, list):
            for file_name in self.include_top_level_files:
                if file_name in self.files_to_exclude:
                    continue
                file_path = os.path.join(self.repo_root, file_name)
                if os.path.exists(file_path) and not self.should_exclude_file(file_path):
                    try:
                        with open(file_path, 'r', encoding='utf-8') as f:
                            content = f.read()
                        extension = os.path.splitext(file_name)[1]
                        if extension == '.ipynb':
                            content = self.convert_ipynb_to_md(content)
                            self.store_file_content(content, file_name, True)
                        else:
                            self.store_file_content(content, file_name, False)
                    except Exception as e:
                        print(f"Error reading file {file_path}: {str(e)}")

        # 2) Traverse specified directories
        for dir_to_traverse in self.dirs_to_traverse:
            self.traverse_directory(dir_to_traverse)

        # 3) Include specific user-specified files
        self.include_specific_files()

        # 4) Compute directory line counts
        self.compute_directory_line_counts()

        # 5) Build the directory tree string
        directory_tree_str, _ = self.get_directory_tree(self.repo_root, current_depth=0, lines_word_used=False)

        # 6) Now write the final output with XML-style tags
        with open(self.output_file, 'w', encoding='utf-8') as out:
            out.write("<codebase_context>\n\n")

            # (optional) export config block
            if self.dump_config:
                # We'll label with the config filename if it exists
                # or just 'inline-config' if it doesn't
                config_tag_label = self.config_filename if self.config_filename else "inline-config"
                out.write(f"<repo export config: {config_tag_label}>\n")
                # For debugging, we can dump a JSON version of the object's fields.
                # Typically you'd store something more minimal. 
                # We'll replicate the older logic:
                config_data = {
                    # self.* variables that might be relevant to see
                    "repo_root": self.repo_root,
                    "export_name": self.export_name,
                    "dirs_to_traverse": self.dirs_to_traverse,
                    "include_top_level_files": self.include_top_level_files,
                    "included_extensions": self.included_extensions,
                    "subdirs_to_exclude": self.subdirs_to_exclude,
                    "files_to_exclude": self.files_to_exclude,
                    "depth": self.depth,
                    "dump_config": self.dump_config,
                    "exhaustive_dir_tree": self.exhaustive_dir_tree,
                    "blacklisted_dirs": self.blacklisted_dirs,
                    "blacklisted_files": self.blacklisted_files,
                    "files_to_include": self.files_to_include,
                    "always_exclude_patterns": self.always_exclude_patterns,
                    "dirs_for_tree": self.dirs_for_tree
                }
                out.write(json.dumps(config_data, indent=2))
                out.write(f"\n</repo export config: {config_tag_label}>\n\n")

            # Directory tree block
            out.write(f"<dirtree: {self.repo_root}>\n")
            out.write(directory_tree_str)
            out.write(f"</dirtree: {self.repo_root}>\n\n")

            # Files
            for (rel_path, content, ipynb_converted) in self.file_contents:
                out.write(f"<file: {rel_path}>\n")
                if ipynb_converted:
                    out.write("(NOTE: ipynb notebook converted to md)\n")
                out.write(content.rstrip('\n'))
                out.write(f"\n</file: {rel_path}>\n\n")

            out.write("</codebase_context>\n")

        # Console summary
        print(f"Exported to: {self.output_file}")
        print(f"Total number of lines: {self.total_lines}")
        print("Number of exported files by extension:")
        for ext, count in self.exported_files_count.items():
            print(f"  {ext}: {count}")

def get_base_path() -> str:
    """
    Determine the base path based on the host platform or command-line argument.
    """
    if '--pop' in sys.argv:
        return '/home/caleb/Documents/GitHub/'  # pop-xps popOS system
    elif platform.system() == "Darwin":  # macOS
        return "/Users/caleb/Documents/GitHub"
    elif platform.system() == "Windows":  # Windows
        return r"C:\Users\front\Documents\GitHub"
    else:  # Linux or other (dev server, or polliserve instances)
        return "/home/caleb/repo"

def load_config(config_filename: str) -> dict:
    """
    Load configuration from a JSON file located in `utils/export_repo/configs`,
    normalizing all paths inside.
    """
    base_path = get_base_path()
    config_path = os.path.join(base_path, "utils/export_repo/configs", config_filename)
    config_path = PathConverter.to_system_path(config_path)
    
    with open(config_path, 'r', encoding='utf-8') as config_file:
        config = json.load(config_file)
    
    # Normalize
    config = PathConverter.normalize_config_paths(config)
    return config

def get_default_config(repo_root: str) -> dict:
    """
    Provide a default config if the user passes a directory path 
    instead of a config file.
    """
    return {
        'repo_root': repo_root,
        'export_name': f"{os.path.basename(repo_root)}_export.txt",
        'dirs_to_traverse': ['.'],
        'include_top_level_files': 'all',
        'included_extensions': 'all',
        'subdirs_to_exclude': ['__pycache__'],
        'files_to_exclude': [],
        'depth': 10,
        'exhaustive_dir_tree': False,
        'files_to_include': [],
        'always_exclude_patterns': ['export.txt'],
        'dump_config': False
    }

def main():
    """
    Command-line entry point.
    Usage:
      python export_repo_to_txt.py [--pop] [--dump-config] <config_filename|repo_root>
    """
    args = sys.argv[1:]
    config_filename = None
    pop_flag = False
    dump_config_flag = False

    for arg in args:
        if arg == '--pop':
            pop_flag = True
        elif arg == '--dump-config':
            dump_config_flag = True
        elif not arg.startswith('--'):
            config_filename = arg

    if not config_filename:
        print("Usage: python export_repo_to_txt.py [--pop] [--dump-config] <config_filename or repo_root>")
        sys.exit(1)

    # Distinguish whether the user gave us a directory or a config file
    if os.path.isdir(config_filename):
        # Use a default config if a directory was passed
        config = get_default_config(config_filename)
        # We'll embed the actual path in config_filename for labeling if we want
        config_filename_label = f"default-for-{os.path.basename(config_filename)}"
    else:
        # If not a directory, treat it as a config file name
        if not config_filename.endswith('.json'):
            config_filename += '.json'
        config = load_config(config_filename)
        config_filename_label = config_filename

    # If pop_flag was set, do any path transformations
    if pop_flag:
        base_path = '/home/caleb/Documents/GitHub/'
        config['repo_root'] = config['repo_root'].replace("/home/caleb/repo", base_path)

    # If --dump-config was requested, set in config
    config['dump_config'] = dump_config_flag

    # Create and run exporter
    exporter = RepoExporter(config, config_filename=config_filename_label)
    exporter.export_repo()

if __name__ == "__main__":
    main()
</file: export_repo_to_txt.py>

<file: configs/polliFormer-COPAP.json>
{
    "repo_root": "/home/caleb/repo/polliFormer/polliFormer",
    "export_name": "polliFormer_copap_export.txt",
    "delimiter": "----",
    "dirs_to_traverse": ["h5data", "aug"],
    "include_top_level_files": "none",
    "included_extensions": [".py", ".yaml"],
    "files_to_include": ["main.py", "config.py", "docs/data/copap.md"],
    "files_to_exclude": ["main_v0.py", "main_v1.py", "completed_plans.md"],
    "always_exclude_patterns": ["export.txt"],
    "exhaustive_dir_tree": false
  }
</file: configs/polliFormer-COPAP.json>

<file: configs/ibridaDB_v0rX.json>
{
    "repo_root": "/home/caleb/repo/ibridaDB/dbTools",
    "export_name": "ibridaDB_v0rX_export.txt",
    "delimiter": "----",
    "dirs_to_traverse": ["ingest/v0", "export/v0"],
    "files_to_include": ["/home/caleb/repo/ibridaDB/docker/stausee/docker-compose.yml", "/home/caleb/repo/ibridaDB/docker/stausee/entrypoint.sh",
    "/home/caleb/repo/ibridaDB/dbTools/README.md", "/home/caleb/repo/ibridaDB/dbTools/FLOW.md", "/home/caleb/repo/ibridaDB/dbTools/schema.md"],
    "include_top_level_files": "all",
    "included_extensions": [".sh", ".md", ".sql"],
    "always_exclude_patterns": ["export.txt"],
    "exhaustive_dir_tree": false
  }
</file: configs/ibridaDB_v0rX.json>

<file: configs/metaformer1.json>
{
  "repo_root": "/home/caleb/repo/Polli-Brain/metaformer",
  "export_name": "metaformer_repo_export.txt",
  "delimiter": "----",
  "dirs_to_traverse": ["models"],
  "include_top_level_files": "all",
  "included_extensions": [".py", ".yaml", ".md"],
  "exhaustive_dir_tree": false,
  "files_to_include": ["experiment_log.md", "base2.yaml"]
}
</file: configs/metaformer1.json>

<file: configs/polliFormer-configData.json>
{
    "repo_root": "/home/caleb/repo/polliFormer/polliFormer",
    "export_name": "polliFormer_configData_export.txt",
    "delimiter": "----",
    "dirs_to_traverse": ["h5data", "ibrida", "loss", "utils"],
    "dirs_for_tree": ["polliFormer", "configs/experiments"],
    "include_top_level_files": ["config.py", "main.py"],
    "included_extensions": [".py"],
    "files_to_include": ["polliFormer/docs/data/copap.md"],
    "always_exclude_patterns": ["export.txt", "224.yaml", ".bak.py"],
    "exhaustive_dir_tree": false
  }
</file: configs/polliFormer-configData.json>

<file: configs/polliFormer-autoresume.json>
{
    "repo_root": "/home/caleb/repo/polliFormer/polliFormer",
    "export_name": "polliFormer-autoresume_export.txt",
    "delimiter": "----",
    "dirs_to_traverse": [],
    "dirs_to_exclude": [],
    "include_top_level_files": "all",
    "included_extensions": [".py", ".yaml"],
    "files_to_exclude": ["export.py", "console_ui.py"],
    "files_to_include": ["utils/autobatch.py", "utils/wandb.py", "utils/checkpoint.py", "utils/model_utils.py", "utils/config_utils.py",
    "utils/distributed.py", "utils/metrics/tracker.py", "utils/backblaze.py",
        "/home/caleb/repo/polliFormer/configs/experiments/tests/blade_amphibia_hybrid_ft_sm.yaml"],
    "always_exclude_patterns": ["export.txt"],
    "exhaustive_dir_tree": true
  }
</file: configs/polliFormer-autoresume.json>

<file: configs/polliFormer-modelsSlim.json>
{
    "repo_root": "/home/caleb/repo/polliFormer/polliFormer",
    "export_name": "polliFormer-modelsSlim_export.txt",
    "delimiter": "----",
    "dirs_to_traverse": [],
    "include_top_level_files": "none",
    "included_extensions": [".py", ".yaml"],
    "files_to_include": ["models/mFormerV0.py", "models/build.py", "models/blocks/mb_conv.py", "models/blocks/relative_mhsa.py", "models/blocks/drop_path.py",
"/home/caleb/repo/polliFormer/configs/model/archs/mFormerV0/mFormerV0_sm.yaml",
    "/home/caleb/repo/polliFormer/configs/experiments/tests/blade_amphibia_hybrid_ft_sm_2.yaml",
    "/home/caleb/repo/polliFormer/configs/model/archs/mFormerV0/mFormerV0_md.yaml", "/home/caleb/repo/polliFormer/configs/model/archs/mFormerV0/mFormerV0_lg.yaml",
    "utils/checkpoint.py", "utils/model_utils.py",
    "/home/caleb/repo/polliFormer/dev/papers/Diao et al. - 2022 - MetaFormer A Unified Meta Framework for Fine-Grai.md",
    "/home/caleb/repo/polliFormer/dev/notes/model/mFormerV1/PRD.md"],
    "files_to_exclude": ["export.py", "enhanced_mb_conv.py", "console_ui.py"],
    "always_exclude_patterns": ["export.txt"],
    "exhaustive_dir_tree": true
  }
</file: configs/polliFormer-modelsSlim.json>

<file: configs/h5merge-mini.json>
{
    "repo_root": "/home/caleb/repo/ibrida/ibrida/hdf5/merge",
    "export_name": "hdf5_merge_mini_export.txt",
    "delimiter": "----",
    "dirs_to_traverse": [],
    "files_to_include": [],
    "files_to_exclude": ["cli.py", "h5_utils.py","config.py","input_diagnostics.py", "output_diagnostics.py", "manual_verify.py", "split_adjustment.tmp.md"],
    "include_top_level_files": "all",
    "included_extensions": [".py"],
    "always_exclude_patterns": ["export.txt"],
    "exhaustive_dir_tree": false
  }
</file: configs/h5merge-mini.json>

<file: configs/ezprompt.json>
{
    "repo_root": "/home/caleb/repo/ezprompt",
    "export_name": "ezprompt_export.txt",
    "delimiter": "----",
    "dirs_to_traverse": ["ezprompt"],
    "subdirs_to_exclude": [".dev"],
    "include_top_level_files": "all",
    "included_extensions": [".py", ".env", ".toml", ".md"],
    "always_exclude_patterns": ["uv.lock", "export.txt", ".log", ".venv", ".gitignore"],
    "exhaustive_dir_tree": false
  }
</file: configs/ezprompt.json>

<file: configs/polliFormer-classification0.json>
{
    "repo_root": "/home/caleb/repo/polliFormer/polliFormer",
    "export_name": "polliFormer_classification0_export.txt",
    "delimiter": "----",
    "dirs_to_traverse": ["loss", "models/heads"],
    "include_top_level_files": "all",
    "included_extensions": [".py"],
    "files_to_include": [
        "utils/param_filters.py",
        "h5data/build.py",
        "h5data/vectorized_dataset_processor.py",
        "h5data/prefetching_hybrid_dataset.py",
    "models/mFormerV0.py", "models/build.py", "models/model_factory.py", "models/base_model.py",
    "/home/caleb/repo/polliFormer/configs/experiments/tests/blade_amphibia_mini_0_conditional.yaml",
    "/home/caleb/repo/polliFormer/configs/experiments/tests/blade_amphibia_mini_0_hsoftmax.yaml",
    "/home/caleb/repo/polliFormer/configs/model/archs/mFormerV0/mFormerV0_sm.yaml"],
    "files_to_exclude": ["console_ui.py", "backblaze.py", "optimizer.py", "config.py"],
    "subdirs_to_exclude": ["utils/inference"],
    "always_exclude_patterns": ["export.txt"],
    "exhaustive_dir_tree": true
  }
</file: configs/polliFormer-classification0.json>

<file: configs/polliFormer-train.json>
{
  "repo_root": "/home/caleb/repo/polliFormer/polliFormer",
  "export_name": "polliFormer_train_export.txt",
  "delimiter": "----",
  "dirs_to_traverse": ["loss"],
  "include_top_level_files": "all",
  "included_extensions": [".py"],
  "files_to_include": ["/home/caleb/repo/polliFormer/configs/experiments/tests/blade_amphibia_hybrid_ft_sm_9.yaml",
  "h5data/build.py", "h5data/base_prefetching_dataset.py", "h5data/prefetching_hybrid_dataset.py", "h5data/h5dataloader.py", "h5data/prefetching_h5_dataset.py", "h5data/h5dataloader.py",
  "aug/cpu/selective_mixup.py", "aug/gpu/selective_mixup.py",
  "models/mFormerV0.py"],
  "files_to_exclude": ["console_ui.py", "dataset_metadata.py", "config_utils.py"],
  "subdirs_to_exclude": ["configs/legacy", "utils/inference"],
  "always_exclude_patterns": ["export.txt"],
  "exhaustive_dir_tree": true
}
</file: configs/polliFormer-train.json>

<file: configs/polliFormer-blade-angio-0.json>
{
    "repo_root": "/home/caleb/repo/polliFormer",
    "export_name": "polliFormer_blade_angio_0_export.txt",
    "delimiter": "----",
    "dirs_to_traverse": [],
    "include_top_level_files": "none",
    "included_extensions": [".py", ".yaml", ".md"],
    "files_to_include": ["/home/caleb/repo/polliFormer/configs/experiments/tests/blade_angio_0.yaml", "/home/caleb/repo/polliFormer/configs/models/mFormer/mFormerV0_0_heteroHA_TS.yaml",
    "/home/caleb/repo/polliFormer/configs/model/archs/mFormerV0/mFormerV0_0.yaml", "/home/caleb/repo/polliFormer/configs/model/archs/mFormerV0/mFormerV0.yaml",
"/home/caleb/repo/polliFormer/configs/models/mFormer/mFormerV0_0_heteroHA_TS.yaml", "/home/caleb/repo/polliFormer/configs/model/components/classification_heads/L1020304050_linear.yaml"],
    "files_to_exclude": [],
    "always_exclude_patterns": ["export.txt", ".egg-info", "__pycache__", ".DS_Store"],
    "exhaustive_dir_tree": false
  }
</file: configs/polliFormer-blade-angio-0.json>

<file: configs/hFormer0-serve.json>
{
    "repo_root": "/home/caleb/repo/polliFormer/torchserve/hFormer0",
    "export_name": "polliFormer_serve_export.txt",
    "delimiter": "----",
    "dirs_to_traverse": ["."],
    "include_top_level_files": "all",
    "included_extensions": [".py", ".yaml", ".properties", ".sh", ".xml", ".txt"],
    "subdirs_to_exclude": ["pkgs", "test"],
    "exhaustive_dir_tree": false
  }
</file: configs/hFormer0-serve.json>

<file: configs/polliFormer-paramFilters.json>
{
  "repo_root": "/home/caleb/repo/polliFormer/polliFormer",
  "export_name": "polliFormer_paramFilters_export.txt",
  "delimiter": "----",
  "dirs_to_traverse": ["loss","components", "utils/logging"],
  "include_top_level_files": "all",
  "included_extensions": [".py"],
  "files_to_include": [ "/home/caleb/repo/polliFormer/configs/experiments/tests/blade_amphibia_hybrid_ft_sm_14.yaml", 
"utils/param_filters.py", "utils/distributed.py", "utils/checkpoint.py", "utils/model_utils.py",
"models/mFormerV0.py", "models/build.py", "models/blocks/mb_conv.py", "models/blocks/relative_mhsa.py", "models/blocks/drop_path.py",
  "/home/caleb/repo/polliFormer/style_guide/metrics.md"],
  "files_to_exclude": ["console_ui.py", "backblaze.py"],
  "subdirs_to_exclude": ["utils/inference"],
  "always_exclude_patterns": ["export.txt"],
  "exhaustive_dir_tree": true
}
</file: configs/polliFormer-paramFilters.json>

<file: configs/polliFormer-buildData.json>
{
    "repo_root": "/home/caleb/repo/polliFormer/polliFormer",
    "export_name": "polliFormer_buildData_export.txt",
    "delimiter": "----",
    "dirs_to_traverse": ["h5data"],
    "dirs_for_tree": ["polliFormer", "configs/experiments"],
    "include_top_level_files": ["config.py", "main.py"],
    "files_to_exclude": ["h5data/dataset_processor.py"],
    "included_extensions": [".py", ".yaml"],
    "always_exclude_patterns": ["export.txt", "224.yaml", ".bak.py"],
    "exhaustive_dir_tree": false
  }
</file: configs/polliFormer-buildData.json>

<file: configs/polliFormer-autobatch.json>
{
  "repo_root": "/home/caleb/repo/polliFormer/polliFormer",
  "export_name": "polliFormer_autobatch_export.txt",
  "delimiter": "----",
  "dirs_to_traverse": ["utils"],
  "include_top_level_files": "all",
  "included_extensions": [".py"],
  "files_to_include": ["utils/wandb.py", "/home/caleb/repo/polliFormer/configs/experiments/tests/blade_amphibia_hybrid.yaml"],
  "files_to_exclude": ["dataset_processor.py", "console_ui.py", "backblaze.py"],
  "subdirs_to_exclude": ["utils/inference"],
  "always_exclude_patterns": ["export.txt"],
  "exhaustive_dir_tree": true
}
</file: configs/polliFormer-autobatch.json>

<file: configs/h5merge.json>
{
    "repo_root": "/home/caleb/repo/ibrida/ibrida/hdf5/merge",
    "export_name": "hdf5_merge_export.txt",
    "delimiter": "----",
    "dirs_to_traverse": ["validation"],
    "files_to_include": ["/home/caleb/repo/ibrida/ibrida/hdf5/merge_tmp/h5dump_first_train.txt", "/home/caleb/stausee-report.txt"],
    "files_to_exclude": ["manual_verify.py", "split_adjustment.tmp.md"],
    "include_top_level_files": "all",
    "included_extensions": [".py", ".md"],
    "always_exclude_patterns": ["export.txt"],
    "exhaustive_dir_tree": false
  }
</file: configs/h5merge.json>

<file: configs/ibridaDB_v0r1_ingest.json>
{
    "repo_root": "/home/caleb/repo/ibridaDB/dbTools/ingest/v0",
    "export_name": "ibridaDB_v0r1_ingest.txt",
    "delimiter": "----",
    "dirs_to_traverse": ["common", "r1", "utils"],
    "include_top_level_files": "all",
    "included_extensions": [".sh", ".sql"],
    "always_exclude_patterns": ["export.txt", ".log", "wrapper_*"],
    "exhaustive_dir_tree": false
  }
</file: configs/ibridaDB_v0r1_ingest.json>

<file: configs/wandb_sweep.json>
{
  "repo_root": "/home/caleb/repo/wandb_docs/content/guides/models/sweeps",
  "export_name": "wandb_sweep_export.txt",
  "delimiter": "----",
  "dirs_to_traverse": ["define-sweep-configuration"],
  "include_top_level_files": "all",
  "included_extensions": [".md"],
  "files_to_exclude": ["logger.py"],
  "subdirs_to_exclude": ["configs/legacy"],
  "always_exclude_patterns": ["export.txt"],
  "exhaustive_dir_tree": true
}
</file: configs/wandb_sweep.json>

<file: configs/polliFormer-DynSlim.json>
{
  "repo_root": "/home/caleb/repo/polliFormer/polliFormer",
  "export_name": "polliFormer_DynSlim_export.txt",
  "delimiter": "----",
  "dirs_to_traverse": ["components"],
  "include_top_level_files": "none",
  "included_extensions": [".py"],
  "files_to_include": ["lr_scheduler.py", "optimizer.py"],
  "files_to_exclude": ["console_ui.py", "backblaze.py"],
  "subdirs_to_exclude": ["utils/inference"],
  "always_exclude_patterns": ["export.txt"],
  "exhaustive_dir_tree": true
}
</file: configs/polliFormer-DynSlim.json>

<file: configs/polliFormer-tests.json>
{
    "repo_root": "/home/caleb/repo/polliFormer/polliFormer",
    "export_name": "polliFormer_tests_export.txt",
    "delimiter": "----",
    "dirs_to_traverse": ["aug", "h5data", "models", "loss", "utils", "components", "ibrida", "evaluation", "serving", "tests"],
    "include_top_level_files": "all",
    "included_extensions": [".py"],
    "files_to_include": ["copap.md", "inheritance.md", "model.md", "training.md"],
    "files_to_exclude": ["main_v0.py", "main_v1.py", "completed_plans.md"],
    "always_exclude_patterns": ["export.txt"],
    "exhaustive_dir_tree": false
  }
</file: configs/polliFormer-tests.json>

<file: configs/model-explorer.json>
{
    "repo_root": "/home/caleb/repo/Polli-Brain/model-explorer",
    "export_name": "model-explorer-repo-export.txt",
    "delimiter": "----",
    "dirs_to_traverse": ["example_colabs", "src", "model-explorer.wiki"],
    "include_top_level_files": "all",
    "included_extensions": [".py", ".yaml", ".md"],
    "exhaustive_dir_tree": false,
    "subdirs_to_exclude": ["server"],
    "depth": 10
  }
  
  
</file: configs/model-explorer.json>

<file: configs/ibrida.json>
{
    "repo_root": "/home/caleb/repo/ibrida/src/ibrida",
    "export_name": "ibrida_export.txt",
    "delimiter": "----",
    "dirs_to_traverse": ["generator", "types"],
    "files_to_include": ["/home/caleb/repo/ibrida/pyproject.toml", "/home/caleb/repo/ibrida/README.md",
    "/home/caleb/repo/ibrida/extra/generator/configs/prod/pta_non_rg_v0r1_hybrid.json"],
    "files_to_exclude": [],
    "include_top_level_files": ["pyproject.toml", "README.md"],
    "included_extensions": [".py"],
    "always_exclude_patterns": ["export.txt"],
    "exhaustive_dir_tree": true
  }
</file: configs/ibrida.json>

<file: configs/polliFormer-configModelMini.json>
{
    "repo_root": "/home/caleb/repo/polliFormer",
    "export_name": "polliFormer_configModelMini_export.txt",
    "delimiter": "----",
    "dirs_to_traverse": ["configs"],
    "subdirs_to_exclude": [],
    "include_top_level_files": "all",
    "included_extensions": [".py", ".yaml"],
    "files_to_include": ["/home/caleb/repo/polliFormer/polliFormer/config.py", "/home/caleb/repo/polliFormer/polliFormer/utils/config_utils.py"],
    "files_to_exclude": [],
    "always_exclude_patterns": ["export.txt", ".egg-info", "__pycache__", ".DS_Store"],
    "exhaustive_dir_tree": false
  }
</file: configs/polliFormer-configModelMini.json>

<file: configs/export_repo.json>
{
    "repo_root": "/home/caleb/repo/utils/export_repo",
    "export_name": "export_repo_export.txt",
    "delimiter": "----",
    "dirs_to_traverse": ["configs"],
    "include_top_level_files": "all",
    "files_to_include": [
    "/home/caleb/repo/utils/README.md"
    ],
    "included_extensions": [".py", ".json"],
    "always_exclude_patterns": ["export.txt"],
    "exhaustive_dir_tree": false
  }
</file: configs/export_repo.json>

<file: configs/polliOS.json>
{
    "repo_root": "/home/caleb/repo/polliOS-core/PolliOS",
    "export_name": "polliOS-core-repo-export.txt",
    "delimiter": "----",
    "dirs_to_traverse": ["PolliOS"],
    "include_top_level_files": "none",
    "included_extensions": [".py", ".yml", ".md"],
    "exhaustive_dir_tree": false,
    "subdirs_to_exclude": ["updaters", "utils"],
    "depth": 10
  }
  
  
</file: configs/polliOS.json>

<file: configs/polliFormer-utils.json>
{
    "repo_root": "/home/caleb/repo/polliFormer/polliFormer",
    "export_name": "polliFormer-utils_export.txt",
    "delimiter": "----",
    "dirs_to_traverse": ["utils"],
    "include_top_level_files": "none",
    "included_extensions": [".py", ".yaml"],
    "files_to_exclude": ["export.py"],
    "always_exclude_patterns": ["export.txt"],
    "exhaustive_dir_tree": true
  }
</file: configs/polliFormer-utils.json>

<file: configs/polliFormer-modelsPrunedInv-codeOnly.json>
{
    "repo_root": "/home/caleb/repo/polliFormer/polliFormer/models",
    "export_name": "polliFormer_modelsPrunedInv_codeOnly_export.txt",
    "delimiter": "----",
    "dirs_to_traverse": ["blocks", "components", "normalization", "resolvers"],
    "include_top_level_files": "all",
    "included_extensions": [".py", ".yaml"],
    "files_to_exclude": ["export.py"],
    "always_exclude_patterns": ["export.txt"],
    "exhaustive_dir_tree": false
  }
</file: configs/polliFormer-modelsPrunedInv-codeOnly.json>

<file: configs/metaformer2.json>
{
  "repo_root": "/home/caleb/repo/Polli-Brain/metaformer",
  "export_name": "metaformer_repo_export.txt",
  "delimiter": "----",
  "dirs_to_traverse": ["models", "data"],
  "include_top_level_files": "all",
  "included_extensions": [".py", ".yaml"],
  "exhaustive_dir_tree": false
}
</file: configs/metaformer2.json>

<file: configs/bulk_dl.json>
{
  "repo_root": "/home/caleb/repo/ibrida/ibrida",
  "export_name": "ibrida_export.txt",
  "delimiter": "----",
  "dirs_to_traverse": ["s3/download", "s3/download/utils"],
  "files_to_include": ["s3_bulk_dl_v6.py", "b2.py", "config.py", "file.py", "graceful_shutdown.py", "init.py", 
  "monitoring.py", "processing.py", "internet_connectivity_checker.py", "csv_schema.py", "paths.py", "chunk_postprocessor.py"],
  "include_top_level_files": "all",
  "included_extensions": [".py", ".yaml", ".properties", ".sh", ".xml"],
  "subdirs_to_exclude": ["pkgs", "test", "fo_dataset", "utils"],
  "files_to_exclude": ["convert_multitask_dataset_to_mmlab2.py"],
  "always_exclude_patterns": ["export.txt", "other_pattern.txt"],
  "exhaustive_dir_tree": false
}
</file: configs/bulk_dl.json>

<file: configs/polliFormer-gradnormSlim.json>
{
    "repo_root": "/home/caleb/repo/polliFormer/polliFormer",
    "export_name": "polliFormer_gradnormSlim_export.txt",
    "delimiter": "----",
    "dirs_to_traverse": ["loss"],
    "include_top_level_files": "all",
    "included_extensions": [".py"],
    "files_to_include": [
        "utils/param_filters.py",
    "models/mFormerV0.py", "models/heads/utils.py",
    "/home/caleb/repo/polliFormer/configs/experiments/tests/blade_amphibia_hybrid_ft_sm_11.yaml"],
    "files_to_exclude": ["console_ui.py", "backblaze.py", "optimizer.py", "config.py"],
    "subdirs_to_exclude": ["utils/inference"],
    "always_exclude_patterns": ["export.txt"],
    "exhaustive_dir_tree": true
  }
</file: configs/polliFormer-gradnormSlim.json>

<file: configs/polliFormer-gradnorm2.json>
{
  "repo_root": "/home/caleb/repo/polliFormer/polliFormer",
  "export_name": "polliFormer_gradnorm2_export.txt",
  "delimiter": "----",
  "dirs_to_traverse": ["loss", "ops_schedule", "lr_schedulers"],
  "include_top_level_files": "all",
  "included_extensions": [".py", ".md"],
  "files_to_include": [
    "optimizers/build.py",
    "optimizers/multi_optimizer.py",
    "utils/schedule_utils.py",
    "utils/logging/wandb.py",
    "utils/metrics/tracker.py",
    "utils/metrics/metrics.py",
    "utils/metrics/step_metrics_logger.py",
    "/home/caleb/repo/polliFormer/style_guide/metrics.md",
    "/home/caleb/repo/polliFormer/style_guide/schedule_parameters.md",
    "/home/caleb/repo/polliFormer/docs/training/scheduling.md",
    "/home/caleb/repo/polliFormer/docs/training/metrics.md",
    "/home/caleb/repo/polliFormer/docs/evaluation/validation.md",
    "/home/caleb/repo/polliFormer/docs/evaluation/metrics.md"
  ],
  "files_to_exclude": ["console_ui.py", "backblaze.py"],
  "subdirs_to_exclude": ["utils/inference"],
  "always_exclude_patterns": ["export.txt"],
  "exhaustive_dir_tree": true
}
</file: configs/polliFormer-gradnorm2.json>

<file: configs/polliFormer-J25_0e0.json>
{
    "repo_root": "/home/caleb/repo/polliFormer",
    "export_name": "polliFormer_buildData_export.txt",
    "delimiter": "----",
    "dirs_to_traverse": ["polliFormer/h5data"],
    "dirs_for_tree": ["polliFormer", "configs/experiments"],
    "include_top_level_files": ["config.py", "main.py"],
    "included_extensions": [".py", ".yaml"],
    "files_to_include": ["polliFormer/utils/wandb.py", "polliFormer/utils/config_utils.py", "polliFormer/docs/data/copap.md", "configs/models/mFormer/mFormerV0_0_heteroHA_TS.yaml",
"configs/archs/mFormerV0/mFormerV0_0.yaml", "configs/archs/mFormerV0/mFormerV0.yaml"],
    "always_exclude_patterns": ["export.txt", "224.yaml", ".bak.py"],
    "exhaustive_dir_tree": false
  }
</file: configs/polliFormer-J25_0e0.json>

<file: configs/polliFormer-loggingSlim.json>
{
  "repo_root": "/home/caleb/repo/polliFormer/polliFormer",
  "export_name": "polliFormer_loggingSlim_export.txt",
  "delimiter": "----",
  "dirs_to_traverse": [],
  "include_top_level_files": "none",
  "included_extensions": [".py"],
  "files_to_include": ["main.py", "ops_schedule.py",
  "utils/wandb.py", "utils/logging_utils.py", "utils/metrics/tracker.py"],
  "files_to_exclude": ["console_ui.py", "backblaze.py", "taxAlign.py"],
  "subdirs_to_exclude": ["utils/inference"],
  "always_exclude_patterns": ["export.txt"],
  "exhaustive_dir_tree": true
}
</file: configs/polliFormer-loggingSlim.json>

<file: configs/sam2_demo.json>
{
    "repo_root": "/home/caleb/repo/sam2/demo",
    "export_name": "sam2_demo_export.txt",
    "delimiter": "----",
    "dirs_to_traverse": ["frontend"],
    "subdirs_to_exclude": ["frontend/node_modules", "frontend/dist", "frontend/public", "frontend/src/assets", "frontend/src/components/ui/icons", "frontend/src/components/ui/icons/icons",
    "frontend/src/common/components/gallery", "frontend/src/common/codecs", "frontend/src/common/tracker", "__generated__"],
    "files_to_include": ["/home/caleb/repo/sam2/docker-compose.yaml", "/home/caleb/repo/sam2/backend.Dockerfile", "/home/caleb/repo/sam2/demo/frontend/frontend.Dockerfile",
    "/home/caleb/repo/sam2/demo/backend/server/app_conf.py", "/home/caleb/repo/sam2/demo/backend/server/app.py"],
    "files_to_exclude": [],
    "include_top_level_files": "all",
    "included_extensions": [".py", ".md", ".ts", ".tsx", ".json", ".html", ".css", ".js", ".yaml", ".yml", ".graphql", ".dockerignore"],
    "always_exclude_patterns": ["export.txt"],
    "exhaustive_dir_tree": false
  }
</file: configs/sam2_demo.json>

<file: configs/polliFormer-serve.json>
{
    "repo_root": "/home/caleb/repo/Polli-Brain/metaformer/torchserve/polliFormer0",
    "export_name": "polliFormer_serve_export.txt",
    "delimiter": "----",
    "dirs_to_traverse": ["."],
    "include_top_level_files": "all",
    "included_extensions": [".py", ".yaml", ".properties", ".sh", ".xml", ".txt"],
    "subdirs_to_exclude": ["pkgs", "test"],
    "exhaustive_dir_tree": false
  }
</file: configs/polliFormer-serve.json>

<file: configs/polliFormer-debugCOPAP.json>
{
    "repo_root": "/home/caleb/repo/polliFormer/polliFormer",
    "export_name": "polliFormer_debugCOPAP_export.txt",
    "delimiter": "----",
    "dirs_to_traverse": ["h5data"],
    "include_top_level_files": "none",
    "included_extensions": [".py"],
    "files_to_include": ["main.py", "/home/caleb/repo/polliFormer/polliFormer/utils/config_utils.py",
     "config.py", "/home/caleb/repo/polliFormer/polliFormer/docs/data/copap.md"],
    "files_to_exclude": ["dataset_processor.py"],
    "always_exclude_patterns": ["export.txt"],
    "exhaustive_dir_tree": false
  }
</file: configs/polliFormer-debugCOPAP.json>

<file: configs/polliFormer-modelsSlim2.json>
{
    "repo_root": "/home/caleb/repo/polliFormer/polliFormer",
    "export_name": "polliFormer-modelsSlim2_export.txt",
    "delimiter": "----",
    "dirs_to_traverse": [],
    "include_top_level_files": "none",
    "included_extensions": [".py", ".yaml"],
    "files_to_include": ["models/mFormerV0.py", "models/build.py", "models/blocks/mb_conv.py", "models/blocks/drop_path.py",
    "models/utils/initialization.py", "models/utils/conversion.py",
        "/home/caleb/repo/polliFormer/configs/experiments/tests/blade_amphibia_hybrid_ft_sm.yaml", "/home/caleb/repo/polliFormer/configs/model/archs/mFormerV0/mFormerV0_sm.yaml",
        "/home/caleb/repo/polliFormer/configs/model/archs/mFormerV0/mFormerV0_md.yaml", "/home/caleb/repo/polliFormer/configs/model/archs/mFormerV0/mFormerV0_lg.yaml",
    "utils/checkpoint.py", "utils/model_utils.py"],
    "files_to_exclude": ["export.py", "enhanced_mb_conv.py", "console_ui.py"],
    "always_exclude_patterns": ["export.txt"],
    "exhaustive_dir_tree": true
  }
</file: configs/polliFormer-modelsSlim2.json>

<file: configs/polliOS-codeOnly.json>
{
    "repo_root": "/home/caleb/repo/polliOS-core/PolliOS",
    "export_name": "polliOS-core-codeOnly-export.txt",
    "delimiter": "----",
    "dirs_to_traverse": ["backend", "brain", "engine", "logger", "polliCLI", "runner", "swarm", "updaters", "utils"],
    "include_top_level_files": "none",
    "included_extensions": [".py"],
    "exhaustive_dir_tree": false,
    "subdirs_to_exclude": ["updaters", "utils", ".git"],
    "depth": 10
  }
  
  
</file: configs/polliOS-codeOnly.json>

<file: configs/ezmd.json>
{
    "repo_root": "/home/caleb/repo/ezmd",
    "export_name": "ezmd_export.txt",
    "delimiter": "----",
    "dirs_to_traverse": ["ezmd"],
    "subdirs_to_exclude": ["dev"],
    "include_top_level_files": "all",
    "included_extensions": [".py", ".env", ".toml", ".md"],
    "always_exclude_patterns": ["uv.lock", "export.txt", ".log", ".venv", ".gitignore"],
    "exhaustive_dir_tree": false
  }
</file: configs/ezmd.json>

<file: configs/polliFormer-Dyn.json>
{
  "repo_root": "/home/caleb/repo/polliFormer/polliFormer",
  "export_name": "polliFormer_Dyn_export.txt",
  "delimiter": "----",
  "dirs_to_traverse": ["loss","ops_schedule", "utils/logging", "utils/metrics"],
  "include_top_level_files": "all",
  "included_extensions": [".py"],
  "files_to_include": ["/home/caleb/repo/polliFormer/configs/experiments/tests/blade_amphibia_hybrid_ft_sm_19.yaml",
    "style_guide/metrics.md", "style_guide/schedule_parameters.md",
  "utils/schedule_utils.py", 
"/home/caleb/repo/polliFormer/docs/training/metrics.md",
"/home/caleb/repo/polliFormer/docs/training/scheduling.md",
"/home/caleb/repo/polliFormer/docs/evaluation/validation.md"],
  "files_to_exclude": ["console_ui.py", "backblaze.py"],
  "subdirs_to_exclude": ["utils/inference"],
  "always_exclude_patterns": ["export.txt"],
  "exhaustive_dir_tree": true
}
</file: configs/polliFormer-Dyn.json>

<file: configs/polliFormer-modelsPruned-codeOnly.json>
{
    "repo_root": "/home/caleb/repo/polliFormer/polliFormer/models",
    "export_name": "polliFormer_modelsPruned_codeOnly_export.txt",
    "delimiter": "----",
    "dirs_to_traverse": ["aggregation", "attention", "heads", "utils"],
    "include_top_level_files": "all",
    "included_extensions": [".py", ".yaml"],
    "files_to_exclude": ["export.py"],
    "always_exclude_patterns": ["export.txt"],
    "exhaustive_dir_tree": false
  }
</file: configs/polliFormer-modelsPruned-codeOnly.json>

<file: configs/polliFormer-codeOnly.json>
{
    "repo_root": "/home/caleb/repo/polliFormer/polliFormer",
    "export_name": "polliFormer_codeOnly_export.txt",
    "delimiter": "----",
    "dirs_to_traverse": ["aug", "h5data", "configs", "models", "loss", "utils", "components", "ibrida", "evaluation", "serving"],
    "include_top_level_files": "all",
    "included_extensions": [".py", ".yaml"],
    "files_to_exclude": ["main_v0.py", "main_v1.py", "completed_plans.md"],
    "always_exclude_patterns": ["export.txt"],
    "exhaustive_dir_tree": false
  }
</file: configs/polliFormer-codeOnly.json>

<file: configs/ibrida_analysis.json>
{
    "repo_root": "/home/caleb/repo/ibrida/src/ibrida",
    "export_name": "ibrida_analysis_export.txt",
    "delimiter": "----",
    "dirs_to_traverse": ["generator", "types", "analysis"],
    "files_to_include": ["/home/caleb/repo/ibrida/pyproject.toml", "/home/caleb/repo/ibrida/README.md",
    "/home/caleb/repo/ibrida/extra/generator/configs/prod/pta_non_rg_v0r1_hybrid.json"],
    "files_to_exclude": [],
    "include_top_level_files": ["pyproject.toml", "README.md"],
    "included_extensions": [".py", ".ipynb"],
    "always_exclude_patterns": ["export.txt"],
    "exhaustive_dir_tree": true
  }
</file: configs/ibrida_analysis.json>

<file: configs/polliFormer-loss3.json>
{
  "repo_root": "/home/caleb/repo/polliFormer/polliFormer",
  "export_name": "polliFormer_loss3_export.txt",
  "delimiter": "----",
  "dirs_to_traverse": ["loss", "models/heads"],
  "include_top_level_files": "all",
  "included_extensions": [".py", ".md"],
  "files_to_include": [
    "models/build.py",
    "models/mFormerV0.py",
    "h5data/vectorized_dataset_processor.py",
    "/home/caleb/repo/polliFormer/work/active/Mar17_25/1_taxonomy_label_smoothing_plan.md"
  ],
  "files_to_exclude": ["console_ui.py", "backblaze.py"],
  "subdirs_to_exclude": ["utils/inference"],
  "always_exclude_patterns": ["export.txt"],
  "exhaustive_dir_tree": true
}
</file: configs/polliFormer-loss3.json>

<file: configs/ibrida_autocrop.json>
{
    "repo_root": "/home/caleb/repo/ibrida/src/ibrida",
    "export_name": "ibrida_autocrop_export.txt",
    "delimiter": "----",
    "dirs_to_traverse": ["generator", "types", "/home/caleb/repo/ibrida/deprecated/ibridaV1/s3/postprocess/autocrop"],
    "files_to_include": ["/home/caleb/repo/ibrida/pyproject.toml", "/home/caleb/repo/ibrida/README.md",
    "/home/caleb/repo/ibrida/extra/autocrop/mmdet_context.txt"],
    "files_to_exclude": [],
    "include_top_level_files": ["pyproject.toml", "README.md"],
    "included_extensions": [".py"],
    "always_exclude_patterns": ["export.txt"],
    "exhaustive_dir_tree": true
  }
</file: configs/ibrida_autocrop.json>

<file: configs/polliFormer-modelsDyn.json>
{
  "repo_root": "/home/caleb/repo/polliFormer/polliFormer",
  "export_name": "polliFormer_modelsDyn_export.txt",
  "delimiter": "----",
  "dirs_to_traverse": ["models"],
  "include_top_level_files": "all",
  "included_extensions": [".py"],
  "files_to_include": ["/home/caleb/repo/polliFormer/polliFormer/utils/config_utils.py", "/home/caleb/repo/polliFormer/polliFormer/utils/autobatch.py",
  "/home/caleb/repo/polliFormer/polliFormer/h5data/vectorized_dataset_processor.py", "h5data/h5dataloader.py", "h5data/prefetching_hybrid_dataset.py", "aug/cpu/selective_mixup.py",
"/home/caleb/repo/polliFormer/configs/model/archs/mFormerV0/mFormerV0_sm.yaml", "/home/caleb/repo/polliFormer/configs/model/archs/mFormerV0/mFormerV0_md.yaml",
"/home/caleb/repo/polliFormer/configs/model/archs/mFormerV0/mFormerV0_lg.yaml", "/home/caleb/repo/polliFormer/configs/model/archs/mFormerV0/mFormerV0_xl.yaml"],
  "files_to_exclude": [],
  "subdirs_to_exclude": ["configs/legacy"],
  "always_exclude_patterns": ["export.txt"],
  "exhaustive_dir_tree": true
}
</file: configs/polliFormer-modelsDyn.json>

<file: configs/nextjs.json>
{
    "repo_root": "/home/caleb/repo/polli-labs-mantine/",
    "export_name": "polli_labs_nextjs_repo_export.txt",
    "delimiter": "----",
    "dirs_to_traverse": ["components", "content", "pages", "public", "theme", "types"],
    "include_top_level_files": ["package.json", "tsconfig.json"],
    "included_extensions": [".ts", ".js", ".tsx", ".jsx", ".json"]
  }
</file: configs/nextjs.json>

<file: configs/ibridaDB_v0r1_export.json>
{
    "repo_root": "/home/caleb/repo/ibridaDB/dbTools/export/v0",
    "export_name": "ibridaDB_v0r1_export.txt",
    "delimiter": "----",
    "dirs_to_traverse": ["common", "/home/caleb/repo/ibridaDB/docs"],
    "files_to_include": ["/home/caleb/repo/ibridaDB/docker/Dockerfile", "/home/caleb/repo/ibridaDB/docker/stausee/docker-compose.yml",
    "r1/wrapper_amphibia_all_exc_nonrg_sp_inc_oor_fas_elev.sh", "r1/wrapper_angiospermae_all_exc_nonrg_sp_inc_oor_fas_elev.sh", "r1/wrapper_aves_all_exc_nonrg_sp_inc_oor_fas_elev.sh", "r1/wrapper_mammalia_all_exc_nonrg_sp.sh"],
    "files_to_exclude": ["wrapper_angiospermae_all_exc_nonrg_sp.sh", "wrapper_aves_all_exc_nonrg_sp.sh", "wrapper_mammalia_all_exc_nonrg_sp_inc_oor_fas_elev.sh"],
    "include_top_level_files": "all",
    "included_extensions": [".sh", ".sql", ".md"],
    "always_exclude_patterns": ["export.txt", ".log", "wrapper_*"],
    "exhaustive_dir_tree": false
  }
</file: configs/ibridaDB_v0r1_export.json>

<file: configs/polliFormer-classification1.json>
{
    "repo_root": "/home/caleb/repo/polliFormer/polliFormer",
    "export_name": "polliFormer_classification1_export.txt",
    "delimiter": "----",
    "dirs_to_traverse": ["loss", "models/heads"],
    "include_top_level_files": "all",
    "included_extensions": [".py"],
    "files_to_include": [
        "h5data/build.py",
        "h5data/vectorized_dataset_processor.py",
        "h5data/prefetching_hybrid_dataset.py",
        "utils/dataset_metadata.py",
    "models/mFormerV0.py", "models/build.py", "models/model_factory.py", "models/base_model.py",
    "/home/caleb/repo/polliFormer/configs/experiments/tests/blade_amphibia_mini_0_conditional.yaml",
    "/home/caleb/repo/polliFormer/configs/experiments/tests/blade_amphibia_mini_0_hsoftmax.yaml",
    "/home/caleb/repo/polliFormer/configs/model/archs/mFormerV0/mFormerV0_sm.yaml"],
    "files_to_exclude": ["console_ui.py", "backblaze.py", "optimizer.py", "config.py"],
    "subdirs_to_exclude": ["utils/inference"],
    "always_exclude_patterns": ["export.txt"],
    "exhaustive_dir_tree": true
  }
</file: configs/polliFormer-classification1.json>

<file: configs/polliFormer-data.json>
{
    "repo_root": "/home/caleb/repo/polliFormer/polliFormer",
    "export_name": "polliFormer_data_export.txt",
    "delimiter": "----",
    "dirs_to_traverse": ["h5data", "aug"],
    "files_to_include": ["utils/config_utils.py", "main.py", "config.py", "/home/caleb/repo/polliFormer/configs/experiments/tests/blade_amphibia_hybrid.yaml"],
    "files_to_exclude": ["dataset_processor.py", "console_ui.py", "backblaze.py"],
    "include_top_level_files": "none",
    "included_extensions": [".py"],
    "always_exclude_patterns": ["export.txt", "224.yaml", ".bak.py"],
    "exhaustive_dir_tree": false
  }
</file: configs/polliFormer-data.json>

<file: configs/polliFormer-autoscale.json>
{
  "repo_root": "/home/caleb/repo/polliFormer/polliFormer",
  "export_name": "polliFormer_autoscale_export.txt",
  "delimiter": "----",
  "dirs_to_traverse": ["loss","components", "ops_schedule", "utils/logging", "utils/metrics"],
  "include_top_level_files": "all",
  "included_extensions": [".py"],
  "files_to_include": [ "/home/caleb/repo/polliFormer/configs/experiments/tests/blade_amphibia_hybrid_ft_sm_14.yaml",
  "/home/caleb/repo/polliFormer/polliFormer/h5data/grouped_batch_sampler.py",
  "/home/caleb/repo/polliFormer/polliFormer/h5data/h5_dataloader.py",
    "/home/caleb/repo/polliFormer/polliFormer/utils/schedule_utils.py",
"utils/param_filters.py", "utils/distributed.py", "utils/checkpoint.py",
  "/home/caleb/repo/polliFormer/style_guide/metrics.md", "/home/caleb/repo/polliFormer/style_guide/model_metadata.md",
    "/home/caleb/repo/polliFormer/style_guide/schedule_parameters.md",
    "/home/caleb/repo/polliFormer/work/active/schedule_reform_PRD.md"],
  "files_to_exclude": ["console_ui.py", "backblaze.py"],
  "subdirs_to_exclude": ["utils/inference"],
  "always_exclude_patterns": ["export.txt"],
  "exhaustive_dir_tree": true
}
</file: configs/polliFormer-autoscale.json>

<file: configs/polliFormer-aug.json>
{
    "repo_root": "/home/caleb/repo/polliFormer/polliFormer",
    "export_name": "polliFormer-aug_export.txt",
    "delimiter": "----",
    "dirs_to_traverse": ["aug"],
    "include_top_level_files": "none",
    "included_extensions": [".py"],
    "files_to_exclude": ["export.py"],
    "always_exclude_patterns": ["export.txt"],
    "exhaustive_dir_tree": true
  }
</file: configs/polliFormer-aug.json>

<file: configs/polliFormer-loss2.json>
{
  "repo_root": "/home/caleb/repo/polliFormer/polliFormer",
  "export_name": "polliFormer_loss2_export.txt",
  "delimiter": "----",
  "dirs_to_traverse": ["loss", "models/heads"],
  "include_top_level_files": "all",
  "included_extensions": [".py", ".md"],
  "files_to_include": [
    "models/build.py",
    "models/mFormerV0.py",
    "utils/schedule_utils.py",
    "utils/logging/wandb.py",
    "utils/metrics/tracker.py",
    "utils/metrics/metrics.py",
    "utils/metrics/step_metrics_logger.py"
  ],
  "files_to_exclude": ["console_ui.py", "backblaze.py"],
  "subdirs_to_exclude": ["utils/inference"],
  "always_exclude_patterns": ["export.txt"],
  "exhaustive_dir_tree": true
}
</file: configs/polliFormer-loss2.json>

<file: configs/polliFormer.json>
{
    "repo_root": "/home/caleb/repo/polliFormer/polliFormer",
    "export_name": "polliFormer_export.txt",
    "delimiter": "----",
    "dirs_to_traverse": ["aug", "evaluation", "h5data", "ibrida", "loss", "lr_schedulers", "models", "ops_schedule", "utils",
        "serving", "tests", "tools"
    ],
    "include_top_level_files": "none",
    "included_extensions": [".py"],
    "files_to_exclude": ["README.md", "TODO.md", "mFormerV1.py", "mFormerV1.md", "training.md", "mFormerV1.py", "export.py"],
    "always_exclude_patterns": ["export.txt", ".egg-info", "__pycache__", ".DS_Store"],
    "exhaustive_dir_tree": true
  }
</file: configs/polliFormer.json>

<file: configs/polliFormer-logging.json>
{
  "repo_root": "/home/caleb/repo/polliFormer/polliFormer",
  "export_name": "polliFormer_logging_export.txt",
  "delimiter": "----",
  "dirs_to_traverse": ["utils/metrics"],
  "include_top_level_files": "none",
  "included_extensions": [".py"],
  "files_to_include": ["main.py", "logger.py", "config.py", "ops_schedule.py",
  "utils/wandb.py", "utils/logging_utils.py", "utils/distributed.py"],
  "files_to_exclude": ["console_ui.py", "backblaze.py", "taxAlign.py"],
  "subdirs_to_exclude": ["utils/inference"],
  "always_exclude_patterns": ["export.txt"],
  "exhaustive_dir_tree": true
}
</file: configs/polliFormer-logging.json>

<file: configs/cosm-c360-tools.json>
{
    "repo_root": "/home/caleb/repo/cosm-c360-tools",
    "export_name": "cosm-c360-tools_export.txt",
    "delimiter": "----",
    "dirs_to_traverse": ["src", "docs"],
    "files_to_include": ["/home/caleb/repo/cosm-c360-tools/tests/test_data/ladybird/LADYBIRD.xml", "/home/caleb/repo/cosm-c360-tools/tests/test_data/ladybird/0H/0M/0S/meta.json"],
    "files_to_exclude": ["ts_to_mp4.py", "TODO.md"],
    "include_top_level_files": "all",
    "included_extensions": [".py", ".md", ".toml"],
    "always_exclude_patterns": ["export.txt"],
    "exhaustive_dir_tree": false
  }
</file: configs/cosm-c360-tools.json>

<file: configs/sam2.json>
{
    "repo_root": "/home/caleb/repo/sam2",
    "export_name": "sam2_export.txt",
    "delimiter": "----",
    "dirs_to_traverse": ["notebooks", "sam2", "tools"],
    "files_to_include": [],
    "files_to_exclude": ["CONTRIBUTING.md", "LICENSE", "LICENSE_cctorch", "CODE_OF_CONDUCT.md"],
    "include_top_level_files": "all",
    "included_extensions": [".py", ".md", ".ipynb"],
    "always_exclude_patterns": ["export.txt"],
    "exhaustive_dir_tree": false
  }
</file: configs/sam2.json>

<file: configs/polliFormer-configModel.json>
{
    "repo_root": "/home/caleb/repo/polliFormer",
    "export_name": "polliFormer_configModel_export.txt",
    "delimiter": "----",
    "dirs_to_traverse": ["polliFormer", "configs"],
    "subdirs_to_exclude": ["polliFormer/tests", "polliFormer/serving", "polliFormer/evaluation", "polliFormer/aug",
        "polliFormer/h5data", "polliFormer/logs", "polliFormer/loss"],
    "include_top_level_files": "all",
    "included_extensions": [".py", ".yaml", ".md"],
    "files_to_include": ["/home/caleb/repo/polliFormer/polliFormer/h5data/h5dump_first_train.txt", "/home/caleb/repo/polliFormer/.vscode/launch.json"],
    "files_to_exclude": ["README.md", "TODO.md", "training.md", "mFormerV1.py", "mFormerV1.md", "training.md", "mFormerV1.py"],
    "always_exclude_patterns": ["export.txt", ".egg-info", "__pycache__", ".DS_Store"],
    "exhaustive_dir_tree": false
  }
</file: configs/polliFormer-configModel.json>

<file: configs/cosm-c360-data.json>
{
    "repo_root": "/home/caleb/repo/cosm-c360-tools",
    "export_name": "cosm-c360-data_export.txt",
    "delimiter": "----",
    "dirs_to_traverse": ["/datasets/dataZoo/clients/ladybird/batch_0/raw/"],
    "files_to_include": ["/datasets/dataZoo/clients/ladybird/batch_0/raw/LADYBIRD.xml", "/datasets/dataZoo/clients/ladybird/batch_0/raw/0H/0M/0S/meta.json"],
    "files_to_exclude": ["ts_to_mp4.py", "TODO.md"],
    "include_top_level_files": "none",
    "included_extensions": [".py", ".md", ".toml"],
    "always_exclude_patterns": ["export.txt"],
    "exhaustive_dir_tree": false
  }
</file: configs/cosm-c360-data.json>

<file: configs/polliFormer-hierarchyXL.json>
{
    "repo_root": "/home/caleb/repo/polliFormer/polliFormer",
    "export_name": "polliFormer_classification1_export.txt",
    "delimiter": "----",
    "dirs_to_traverse": ["loss", "models/heads", "h5data", "utils"],
    "include_top_level_files": "all",
    "included_extensions": [".py"],
    "files_to_include": [
    "models/mFormerV0.py", "models/build.py", "models/model_factory.py", "models/base_model.py",
    "/home/caleb/repo/polliFormer/configs/experiments/tests/blade_amphibia_mini_0_conditional.yaml",
    "/home/caleb/repo/polliFormer/configs/experiments/tests/blade_amphibia_mini_0_hsoftmax.yaml",
    "/home/caleb/repo/polliFormer/configs/model/archs/mFormerV0/mFormerV0_sm.yaml"],
    "files_to_exclude": ["console_ui.py", "backblaze.py", "optimizer.py", "config.py"],
    "subdirs_to_exclude": ["utils/inference"],
    "always_exclude_patterns": ["export.txt"],
    "exhaustive_dir_tree": true
  }
</file: configs/polliFormer-hierarchyXL.json>

<file: configs/ladybird_data.json>
{
    "repo_root": "/home/caleb/ladybird_failed_copy",
    "export_name": "ladybird_data_export.txt",
    "delimiter": "----",
    "dirs_to_traverse": ["0H"],
    "subdirs_to_exclude": [],
    "include_top_level_files": "all",
    "included_extensions": [".json", ".xml"],
    "files_to_include": ["/home/caleb/ladybird_failed_copy/LADYBIRD.xml"],
    "files_to_exclude": [],
    "always_exclude_patterns": ["export.txt", ".egg-info", "__pycache__", ".DS_Store", ":Zone.Identifier"],
    "exhaustive_dir_tree": false
  }
</file: configs/ladybird_data.json>

<file: configs/metaformer.json>
{
  "repo_root": "/home/caleb/repo/Polli-Brain/metaformer",
  "export_name": "metaformer_repo_export.txt",
  "delimiter": "----",
  "dirs_to_traverse": ["models"],
  "include_top_level_files": "all",
  "included_extensions": [".py", ".yaml", ".md"],
  "exhaustive_dir_tree": false
}
</file: configs/metaformer.json>

<file: configs/polliFormer-doxDyn.json>
{
  "repo_root": "/home/caleb/repo/polliFormer",
  "export_name": "polliFormer_doxDyn_export.txt",
  "delimiter": "----",
  "dirs_to_traverse": ["docs", "style_guide", "polliFormer/ops_schedule", "polliFormer/loss", "polliFormer/utils"],
  "include_top_level_files": "all",
  "included_extensions": [".py"],
  "files_to_include": ["polliFormer/main.py", "polliFormer/config.py",
    "h5data/build.py","h5data/h5dataloader.py", "h5data/h5_dataloader.py", "h5data/base_prefetching_dataset.py", "h5data/prefetching_hybrid_dataset.py",
  "utils/schedule_utils.py",
    "/home/caleb/repo/polliFormer/configs/experiments/tests/blade_amphibia_hybrid_ft_sm_15.yaml"],
  "files_to_exclude": ["console_ui.py", "backblaze.py", "hpc_utils.py", "dataset_metadata.py", "autobatch.py"],
  "subdirs_to_exclude": ["polliFormer/utils/inference"],
  "always_exclude_patterns": ["export.txt"],
  "exhaustive_dir_tree": false
}
</file: configs/polliFormer-doxDyn.json>

<file: configs/polliFormer-models.json>
{
    "repo_root": "/home/caleb/repo/polliFormer/polliFormer",
    "export_name": "polliFormer-models_export.txt",
    "delimiter": "----",
    "dirs_to_traverse": ["models"],
    "include_top_level_files": "all",
    "included_extensions": [".py", ".yaml"],
    "files_to_include": ["/home/caleb/repo/polliFormer/configs/experiments/tests/blade_amphibia_hybrid.yaml", "/home/caleb/repo/polliFormer/configs/model/archs/mFormerV0/mFormerV0_sm.yaml",
    "/home/caleb/repo/polliFormer/configs/model/archs/mFormerV0/mFormerV0_md.yaml", "/home/caleb/repo/polliFormer/configs/model/archs/mFormerV0/mFormerV0_lg.yaml",
    "/home/caleb/repo/polliFormer/dev/notes/model/mFormerV0/reference_MetaFG_meta_implementation.xml", "/home/caleb/repo/polliFormer/dev/notes/model/mFormerV0/reference_methods.md",
    "/home/caleb/repo/polliFormer/polliFormer/utils/checkpoint.py", "/home/caleb/repo/polliFormer/polliFormer/utils/model_utils.py"],
    "files_to_exclude": ["export.py", "enhanced_mb_conv.py", "console_ui.py"],
    "always_exclude_patterns": ["export.txt"],
    "exhaustive_dir_tree": true
  }
</file: configs/polliFormer-models.json>

<file: configs/h5pull.json>
{
    "repo_root": "/home/caleb/repo/ibrida/ibrida/hdf5",
    "export_name": "h5pull_export.txt",
    "delimiter": "----",
    "dirs_to_traverse": [],
    "files_to_include": ["configs/angio_v0_pop.yaml", "configs/fixer/split0_1.yaml"],
    "include_top_level_files": "all",
    "included_extensions": [".py", ".yaml"],
    "always_exclude_patterns": ["export.txt"],
    "exhaustive_dir_tree": false
  }
</file: configs/h5pull.json>

<file: configs/polliFormer-meta.json>
{
    "repo_root": "/home/caleb/repo/polliFormer/polliFormer",
    "export_name": "polliFormer_meta_export.txt",
    "delimiter": "----",
    "dirs_to_traverse": ["ops_schedule", "utils/metrics", "utils/logging"],
    "files_to_include": ["h5data/build.py", "h5data/vectorized_dataset_processor.py",
        "h5data/h5dataloader.py", "h5data/base_prefetching_dataset.py", "h5data/prefetching_hybrid_dataset.py",
        "utils/meta_utils.py", "utils/schedule_utils.py", "utils/distributed.py",
        "models/mFormerV0.py",
        "/home/caleb/repo/polliFormer/style_guide/metrics.md",
        "/home/caleb/repo/polliFormer/style_guide/model_metadata.md",
        "/home/caleb/repo/polliFormer/style_guide/schedule_parameters.md"],
    "files_to_exclude": ["console_ui.py", "backblaze.py", "autobatch.py"],
    "include_top_level_files": "all",
    "included_extensions": [".py", ".yaml", ".sh"],
    "always_exclude_patterns": ["export.txt", "224.yaml", ".bak.py"],
    "exhaustive_dir_tree": false
  }
</file: configs/polliFormer-meta.json>

<file: configs/autocropper.json>
{
    "repo_root": "/home/caleb/repo/autocropper/src/autocropper",
    "export_name": "autocropper_export.txt",
    "delimiter": "----",
    "dirs_to_traverse": ["utils", "evaluation", "models", "/home/caleb/repo/ibrida/deprecated/ibridaV1/s3/postprocess/autocrop"],
    "files_to_include": ["/home/caleb/repo/autocropper/pyproject.toml", "/home/caleb/repo/autocropper/README.md"],
    "files_to_exclude": [],
    "include_top_level_files": "all",
    "included_extensions": [".py", ".md"],
    "always_exclude_patterns": ["export.txt"],
    "exhaustive_dir_tree": true
  }
</file: configs/autocropper.json>

<file: configs/polliFormer-metrics.json>
{
    "repo_root": "/home/caleb/repo/polliFormer/polliFormer",
    "export_name": "polliFormer_metrics_export.txt",
    "delimiter": "----",
    "dirs_to_traverse": ["utils/metrics", "loss"],
    "subdirs_to_exclude": ["polliFormer/evaluation", "polliFormer/logs", "polliFormer/serving", "polliFormer/tests", "polliFormer/utils/inference", "polliFormer/aug"],
    "include_top_level_files": "all",
    "included_extensions": [".py"],
    "files_to_include": ["/home/caleb/repo/polliFormer/polliFormer/utils/wandb.py"],
    "files_to_exclude": ["README.md", "TODO.md", "mFormerV1.py", "mFormerV1.md", "training.md", "mFormerV1.py", "export.py"],
    "always_exclude_patterns": ["export.txt", ".egg-info", "__pycache__", ".DS_Store"],
    "exhaustive_dir_tree": true,
    "dump_config": true
  }
</file: configs/polliFormer-metrics.json>

<file: configs/polliFormer-loss.json>
{
  "repo_root": "/home/caleb/repo/polliFormer/polliFormer",
  "export_name": "polliFormer_loss_export.txt",
  "delimiter": "----",
  "dirs_to_traverse": ["loss", "models"],
  "include_top_level_files": "all",
  "included_extensions": [".py", ".md"],
  "files_to_include": [
    "utils/schedule_utils.py",
    "utils/logging/wandb.py",
    "utils/metrics/tracker.py",
    "utils/metrics/metrics.py",
    "utils/metrics/step_metrics_logger.py",
    "/home/caleb/repo/polliFormer/style_guide/metrics.md",
    "/home/caleb/repo/polliFormer/style_guide/schedule_parameters.md",
    "/home/caleb/repo/polliFormer/docs/training/scheduling.md",
    "/home/caleb/repo/polliFormer/style_guide/model_metadata.md",
    "/home/caleb/repo/polliFormer/docs/training/metrics.md",
    "/home/caleb/repo/polliFormer/docs/evaluation/validation.md",
    "/home/caleb/repo/polliFormer/docs/evaluation/metrics.md"
  ],
  "files_to_exclude": ["console_ui.py", "backblaze.py"],
  "subdirs_to_exclude": ["utils/inference"],
  "always_exclude_patterns": ["export.txt"],
  "exhaustive_dir_tree": true
}
</file: configs/polliFormer-loss.json>

<file: configs/polliFormer-models-codeOnly.json>
{
    "repo_root": "/home/caleb/repo/polliFormer/polliFormer",
    "export_name": "polliFormer-models-codeOnly_export.txt",
    "delimiter": "----",
    "dirs_to_traverse": ["models"],
    "include_top_level_files": "none",
    "included_extensions": [".py"],
    "files_to_exclude": ["export.py", "mFormerV1.py"],
    "always_exclude_patterns": ["export.txt"],
    "exhaustive_dir_tree": true
  }
</file: configs/polliFormer-models-codeOnly.json>

<file: configs/polliFormer-gradnorm.json>
{
  "repo_root": "/home/caleb/repo/polliFormer/polliFormer",
  "export_name": "polliFormer_gradnorm_export.txt",
  "delimiter": "----",
  "dirs_to_traverse": ["loss", "ops_schedule", "lr_schedulers"],
  "include_top_level_files": "all",
  "included_extensions": [".py", ".md"],
  "files_to_include": [
    "optimizers/build.py",
    "optimizers/multi_optimizer.py",
    "utils/param_filters.py", "utils/unified_filtering.py",
    "utils/schedule_utils.py",
    "models/mFormerV0.py",
    "/home/caleb/repo/polliFormer/style_guide/metrics.md",
    "/home/caleb/repo/polliFormer/style_guide/model_metadata.md",
    "/home/caleb/repo/polliFormer/style_guide/schedule_parameters.md",
    "/home/caleb/repo/polliFormer/docs/training/scheduling.md",
    "/home/caleb/repo/polliFormer/docs/training/metrics.md"

  ],
  "files_to_exclude": ["console_ui.py", "backblaze.py"],
  "subdirs_to_exclude": ["utils/inference"],
  "always_exclude_patterns": ["export.txt"],
  "exhaustive_dir_tree": true
}
</file: configs/polliFormer-gradnorm.json>

<file: configs/polliFormer-gradnormSlim2.json>
{
    "repo_root": "/home/caleb/repo/polliFormer/polliFormer",
    "export_name": "polliFormer_gradnormSlim2_export.txt",
    "delimiter": "----",
    "dirs_to_traverse": ["loss"],
    "include_top_level_files": "all",
    "included_extensions": [".py"],
    "files_to_include": [
    "models/mFormerV0.py", "models/heads/utils.py"],
    "files_to_exclude": ["console_ui.py", "backblaze.py", "optimizer.py", "config.py", "lr_scheduler.py"],
    "subdirs_to_exclude": ["utils/inference"],
    "always_exclude_patterns": ["export.txt"],
    "exhaustive_dir_tree": false
  }
</file: configs/polliFormer-gradnormSlim2.json>

<file: configs/polliFormer-mFormerV1.json>
{
    "repo_root": "/home/caleb/repo/polliFormer/polliFormer",
    "export_name": "polliFormer-mFormerV1_export.txt",
    "delimiter": "----",
    "dirs_to_traverse": [],
    "include_top_level_files": "none",
    "included_extensions": [".py", ".yaml"],
    "files_to_include": ["models/mFormerV0.py", "models/build.py", "models/blocks/mb_conv.py", "models/blocks/relative_mhsa.py", "models/blocks/drop_path.py",
"/home/caleb/repo/polliFormer/configs/model/archs/mFormerV0/mFormerV0_sm.yaml",
    "/home/caleb/repo/polliFormer/configs/model/archs/mFormerV0/mFormerV0_md.yaml", "/home/caleb/repo/polliFormer/configs/model/archs/mFormerV0/mFormerV0_lg.yaml",
    "utils/checkpoint.py", "utils/model_utils.py",
    "/home/caleb/repo/polliFormer/dev/papers/Diao et al. - 2022 - MetaFormer A Unified Meta Framework for Fine-Grai.md",
    "/home/caleb/repo/polliFormer/dev/papers/Heo et al. - 2024 - Rotary Position Embedding for Vision Transformer.md",
    "/home/caleb/repo/polliFormer/dev/notes/model/mFormerV1/PRD.md"],
    "files_to_exclude": ["export.py", "enhanced_mb_conv.py", "console_ui.py"],
    "always_exclude_patterns": ["export.txt"],
    "exhaustive_dir_tree": true
  }
</file: configs/polliFormer-mFormerV1.json>

<file: configs/autocrop.json>
{
    "repo_root": "/home/caleb/repo/ibrida/ibrida/s3/postprocess/autocrop",
    "export_name": "autocrop_export.txt",
    "delimiter": "----",
    "dirs_to_traverse": ["utils", "configs", "models"],
    "include_top_level_files": "all",
    "included_extensions": [".py", ".yaml", ".md"],
    "always_exclude_patterns": ["export.txt"],
    "exhaustive_dir_tree": false
  }
</file: configs/autocrop.json>

</codebase_context>
